


Read your writes is not achievable in a highly available
($1$-available) system. Consider a client that executes the following
two transactions in succession:
\vspace{-.5em}
\begin{align*}
\small
T_1 &: w_x(1)
\\T_2 &: r_x(a)
\end{align*}
If the client executes $T_1$ against a server that is partitioned from
the rest of the other servers, the server must allow $T_1$ to
commit. If the client executes $T_2$ against the same (partitioned)
server, then it will be able to read its writes. However, if the
network topology shifts and the client can only contact a different
server that is partitioned from the server that executed $T_1$, then
the client will be unable to read its own writes and the system will
have to either stall indefinitely to allow the client to read her
writes (violating transactional availability) or will have to
sacrifice read your writes guarantees. However, if the client remains
sticky with the server that executed $T_1$, then we can disallow this
scenario. Accordingly, read your writes, and, by proxy, causal
consistency and PRAM require stickiness. Read your writes is ``free''
in a sticky system, while the remaining causality and PRAM guarantees
can be accomplished with the algorithms for achieving the remaining
session guarantees.

\subsubsection{Additional HAT Guarantees}

In this section, we briefly discuss additional noteworthy guarantees
achievable by HAT systems.

\noindent{\textbf{Consistency}} A HAT system can make limited
consistency guarantees. It can often execute commutative and logically
monotonic~\cite{calm} operations without the risk of invalidating
(also monotonic) application-level integrity constraints. Our goal in
this paper is not to sketch the entire space of consistency models
that are achievable (see Section~\ref{sec:futurework}). We
specifically evaluate TPC-C transaction semantics under HAT
consistency guarantees in Section~\ref{sec:evaluation}.

\vspace{.5em}\noindent{\textbf{Durability}} As we briefly discussed in
Section~\ref{sec:availability}, a client requiring that its
transactions' effects survive $F$ server faults requires at least
($F+1$)-availability.

\vspace{.5em}\noindent{\textbf{Convergence}} To require that the
system propagates writes between replicas, we can require convergence,
or eventual consistency for each data item: in the absence of new
mutations to a data item, in the absence of partitions, all servers
should eventually agree on the value for each item. This is typically
accomplished by any number of anti-entropy protocols, which
periodically update neighboring servers with the latest value for each
data item~\cite{antientropy}. Establishing a final value is related to
determining a total order on transaction updates, as in Read
Uncommitted.

\subsection{Unachievable HAT Semantics}
\label{sec:unachievable-hat}

While there are infinitely many HAT models
(Section~\ref{sec:futurework}), at this point, we have largely
exhausted the range of achievable, previously defined (and useful)
semantics that are available to HAT systems. Before summarizing our
possibility results, we will present impossibility results for HATs,
also defined in terms of previously identified isolation and
consistency anomalies. Perhaps most notably, it is impossible to
prevent Lost Update or Write Skew in a HAT system.

\subsubsection{Unachievable ACID Isolation}

In this section, we demonstrate that preventing Lost Update and Write
Skew---and therefore providing Snapshot Isolation, Repeatable Read,
and one-copy serializability---inherently requires unavailability.

In the words of Berenson et al., \textit{Lost Update} occurs when one
transaction $T1$ reads a given data item, a second transaction $T2$
updates the same data item, then $T1$ modifies the data item based on
its original read of the data item, ``missing'' or ``losing'' $T2$'s
newer update. Consider a database containing only the following
transactions:
\begin{align*}
\small
T_1 &: r_x(a) w_x(a+2)
\\T_2 &: w_x(2)
\end{align*}
If $T_1$ reads $a=1$ but $T_2$'s write to $x$ precedes $T_1$'s write
operation, then the database will end up with $a=3$, a state that
could not have resulted in a serial execution due to from $T_2$'s
``Lost Update.''

It is impossible to prevent Lost Update in a highly available
environment. Consider two clients who submit the following $T_1$ and
$T_2$ on opposite sides of a network partition:
\begin{align*}
\small
T_1 &: r_x(100)~w_x(100+20=120)
\\T_2 &: r_x(100)~w_x(100+30=130)
\end{align*}
Regardless of whether $x=120$ or $x=130$ is chosen by a replica, the
database state could not have arisen serial execution of $T_1$ and
$T_2$.\footnote{In this example, we assume that, as is standard in
  modern databases, replicas accept values as they are written (i.e.,
  register semantics). This particular example could be made
  serializable via the use of commutative updates
  (Section~\ref{sec:eval}) but, as we show in the extended version of
  the paper~\cite{hat-tr}, the problem persists in the general case.}
To prevent this from happening, either $T_1$ or $T_2$ should not have
committed. Each client's respective server might try to detect that
another write occurred, but this requires knowing the version of the
latest write to $x$. In our example, this reduces to a requirement for
linearizability, which is, via Gilbert and Lynch's proof of the CAP
Theorem, provably unachievable with high
availabilty~\cite{gilbert-cap}.

\textbf{Write Skew} is a generalization of Lost Update to multiple
keys. It occurs when one transaction $T1$ reads a given data item $x$,
a second transaction $T2$ reads a different data item $y$, then $T1$
writes to $y$ and commits and $T2$ writes to $x$ and commits. As an
example of Write Skew, consider the following two transactions:
\begin{align*}
\small
T_1 &: r_y(0)~w_x(1)
\\T_2 &: r_x(0)~w_y(1)
\end{align*}
As Berenson et al. describe, if there was an integrity constraint
between $x$ and $y$ such that only one of $x$ or $y$ should have value
$1$ at any given time, then this write skew would violate the constraint (which is preserved in serializable executions). Write skew is a somewhat
esoteric anomaly---for example, it does not appear in
TPC-C~\cite{snapshot-serializable}---but, as a generalization of Lost
Update, it is also unavailable to HATs.

Their need to prevent Lost Update means that Consistent Read, Snapshot
Isolation, and Cursor Stability guarantees are all unavailable.
Repeatable Read (in the sense of Gray~\cite{gray-isolation}, Berenson
et al.~\cite{ansicritique}, or Adya~\cite{adya}) and One-Copy
Serializability need to prevent both Lost Update and Write Skew; this
means that they are also inherently unavailable.

\subsubsection{Unavailable Recency Guarantees}

Data storage systems make various recency guarantees on reads of data
items. As we have discussed, one of the most famous is
linearizability~\cite{herlihy-art}, which states that reads will
return the last completed write to a data item, and there are several
other (weaker) variants such as safe and regular register
semantics. When applied to transactional semantics, the combination of
one-copy serializability and linearizability is called \textit{strong
  (or strict) one-copy serializability}~\cite{adya} (e.g., Google's
Spanner~\cite{spanner}). It is also common, particularly in systems
that allow reading from masters and slaves, to provide a guarantee
such as ``read a version that is no more than five seconds out of
date'' or similar. Unfortunately, an indefinitely long partition can
force an available system to violate any recency bound, so recency
bounds are not enforceable in HAT systems.

\subsection{Summary}
\label{sec:hat-summary}

We summarize our results in Table~\ref{table:hatcompared}. A wide
range of isolation levels are achievable in a HAT systems, including
transactional atomicity, cut isolation, and several session
guarantees. With sticky availability, a system can achieve read your
writes guarantees and PRAM and causal consistency. However, many other
prominent models, such as Snapshot Isolation, One-Copy
Serializability, and Strict Serializability cannot be achieved due to
the inability to prevent Lost Update and Write Skew phenomena.

We illustrate the hierarchy of available, sticky available, and
unavailable consistency models we have discussed in
Figure~\ref{fig:hatcompared}. Many models are simultaneously
achievable, but we find several particularly compelling. If we combine
all sticky-HAT guarantees, we have transactional, causal snapshot
reads. If we combine TA and P-CI, we have transactional snapshot
reads. We can achieve RC, MR, and RYW by simply sticking clients to
servers. And we can also combine unavailable models---for example, an
unavailable system might provide PRAM and One-Copy
Serializability~\cite{daudjee-session}.

To the best of our knowledge, this is the first unification of
database ACID, distributed consistency, and session guarantee
models. Interestingly, strong one-copy serializability subsumes all
other models, while considering the (large) power set of all
compatible models (e.g., the diagram depicts 96 possible highly
available combinations) hints at the vast expanse of consistency
models found in the literature. This taxonomy  is \textit{not}
exhaustive, but we believe it lends substantial clarity into the
relationships between a large subset of the prominent ACID and
distributed consistency models. Additional read/write transaction
semantics that we have omitted should be easily classifiable based on
the available primitives and HAT-incompatible anomaly prevention we
have already discussed.

 \newcommand{\lostupdate}{$^\dagger$}
 \newcommand{\rwskew}{$^\ddagger$}
 \newcommand{\linearizable}{$^\oplus$}

\begin{table}[t!]
\begin{tabular}{| c | p{6cm} | }\hline
HA & Transactional Atomicity (TA), Read Uncommitted (RU), Read
Committed (RC), Item Cut Isolation (P-CI), Predicate Cut Isolation
(P-CI), Monotonic Reads (MR), Monotonic Writes (MW), Writes Follow
Reads (WFR)\\\hline Sticky-HA & Read Your Writes (RYW), PRAM,
Causal\\\hline Unavailable & Cursor Stability (CS)\lostupdate,
Snapshot Isolation (SI)\lostupdate, Repeatable Read
(RR)\lostupdate\rwskew, One-Copy Serializability
(1SR)\lostupdate\rwskew, Recency\linearizable, Safe\linearizable,
Regular\linearizable, Linearizability\linearizable, Strong
1SR\lostupdate\rwskew\linearizable \\\hline
\end{tabular}
\caption{Summary of highly available, sticky highly available, and
  unavailable models considered in this paper. Unavailable models are
  labeled by cause of unavailability: preventing lost
  update\lostupdate, preventing write skew\rwskew, and requiring
  recency guarantees\linearizable.}
\label{table:hatcompared}
\end{table}

\begin{figure}[t!]
\centering
\begin{tikzpicture}[scale=0.8]
  \tikzstyle{sticky}=[rectangle,draw=blue!50,fill=blue!20,thick]
  \tikzstyle{noha}=[ellipse,draw=red!50,fill=red!20,thick, inner sep=0pt,minimum size=12pt]

  \tikzstyle{every node}=[font=\small]

 \node[draw=none,fill=none] (ici) at (1.2, 0) {I-CI};
 \node[draw=none,fill=none] (pci) at (1.65, 1.2) {P-CI};
 \node[draw=none,fill=none] (rc) at (-1.2, .8) {RC};
 \node[draw=none,fill=none] (ru) at (-1.2, 0) {RU};

 \node[draw=none,fill=none] (ta) at (-.2, 1.3) {TA};

 \node[draw=none,fill=none] (mr) at (3.6, 0) {MR};
 \node[draw=none,fill=none] (mw) at (4.8, 0) {MW};
 \node[draw=none,fill=none] (wfr) at (2.4,0) {WFR};
 \node at (6.1,0) [sticky] (ryw) {RYW};

 \node[noha](recency) at (7.7, 0) {recency};
 \node[noha](safe) at (7.7, 1) {safe};
 \node[noha](regular) at (7.7, 2) {regular};
 \node[noha](linearizable) at (7.7, 3) {linearizable};
 \node at (4.8, 2) [sticky] (causal) {causal};
 \node at (4.8, 1) [sticky] (pram) {PRAM};
 \node[noha] (cs) at (-1.2, 1.8) {CS};
 \node[noha] (rr) at (-.2, 2.7) {RR};
 \node[noha] (si) at (2.2, 2.4) {SI};
 \node[noha] (1sr) at (1.2, 3.2) {1SR};
 \node[noha] (ssr) at (3.85, 3.6) {Strong-1SR};

 \draw [->, red] (recency) -- (safe);
 \draw [->, red] (safe) -- (regular);
 \draw [->, red] (regular) -- (linearizable);
 \draw [->, red] (linearizable) -- (ssr);
 \draw [->, red] (1sr) -- (ssr);
 
 \draw [->] (ru) -- (rc);
 \draw [->] (rc) -- (ta);
 \draw [->] (ici) -- (pci);

 \draw [->, blue] (mr) -- (pram);
 \draw [->, blue] (mw) -- (pram);
 \draw [->, blue] (wfr) -- (causal);
 \draw [->, blue] (ryw) -- (pram);
 \draw [->, blue] (pram) -- (causal);

 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (ru) -- (mr);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (rc) -- (ta);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (ici) -- (ta);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (pci) -- (ta);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (rc) -- (mr);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (pci) -- (mr);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (ici) -- (mr);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (ta) -- (ru);
 %\draw[snake=coil, blue, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (ru) -- (causal);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (mr) -- (mw);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (wfr) -- (mw);
 %\draw[snake=coil, blue, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (wfr) -- (ryw);

 \draw [->, red] (rc) -- (cs);
 \draw [->, red] (cs) -- (rr);
 \draw [->, red] (pci) -- (si);
 \draw [->, red] (ici) -- (rr);
 \draw [->, red] (rr) -- (1sr);
 \draw [->, red] (si) -- (1sr);
 \draw [->, red] (ta) -- (si);
 \draw [->, red] (ta) -- (rr);
 \draw [->, red] (causal) -- (linearizable);
 \draw [->, red] (ryw) -- (safe);

\end{tikzpicture}
\label{fig:hat-order}
\caption{HAT, sticky HAT (in boxes), and unavailable models (circled)
  from Table~\protect\ref{table:hatcompared} compared
  graphically. Directed edges represent ordering by model
  strength. Models that do not share a common ancestor can be
  simultaneously achieved, and the resulting availability is that of
  the weakest available model in the combination.}
\label{fig:hatcompared}
\end{figure}


\subsection{Discussion}
\label{sec:discussion}

In this section, we discuss several subleties in our results,
specifically addressing model composition, transactional atomicity
versus linearizability, and stickiness requirements.

\vspace{.5em}\noindent\textbf{Model Composition} Choosing between
combinations of compatible guarantees requires care. Consider the
following transactions:
\begin{align*}
\small
T_1 &: w_x(1)~w_y(1)
\\T_2 &: w_x(2)~w_y(2)
\\T_3 &: r_x(a)~r_y(b)
\end{align*}
If we want to guarantee both cut isolation and transactional atomicity
and the system only executes $T_1$, $T_2$, and $T_3$, then $T_3$ needs
to read $a=b=\bot$, $a=b=1$, or $a=b=2$. This means that either the
implementation should frequently return $\bot$ (definitely undesirable
and possibly non-convergent), keep multiple versions of each data item
(necessitating potentially complicated distributed garbage
collection), or use pre-declared read sets to fetch a consistent cut
of keys before each transaction begins to execute. Using client-side
caching can alleviate some of these challenges~\cite{bolton, swift},
but then the system becomes sticky high available.

Composition cost also varies by combination. For instance, Charron-Bost
has proven that, to capture causality between $N$ communicating
processes, standard vector-based approaches face an upper bound of
$O(N)$ storage per write~\cite{charron-bost}. This means that, with
$100K$ clients, each write might be accompanied by $100K$ timestamps
per vector. This is difficult to scale. By compromising on
availability (e.g., treating a datacenter as a linearizable cluster),
this overhead can be reduced~\cite{cops, eiger}, but it is much
cheaper to provide, say, read your writes, than full causal
consistency.

\vspace{.5em}\noindent\textbf{Linearizability and Transactional
  Atomicity} The relationship between linearizability and
transactional atomicity is non-obvious. Transactional atomicity
dictates that writes to multiple keys across multiple servers are made
visible to readers all at once, while linearizability dictates that
writes to a single key on multiple servers are made visible to all
readers at once---what is different? First, in linearizable (and safe
and regular) systems, writes are made visible to all clients
\textit{immediately} after they finish. With transactional atomicity,
there is no recency guarantee. Second, in linearizabile systems, all
clients see all writes at the same time. With transactional atomicity,
clients may see writes at different times depending on which replicas
they contact. We are not aware of an analogous model in the
distributed systems literature. Accordingly, despite apparent
similarities, transactional atomicity incomparable with and much
cheaper (by availability standards) than linearizability.

\vspace{.5em}\noindent\textbf{Visibility and Stickiness} Sticky
availability can result in much better write \textit{visibility}:
clients will be able to safely read writes more quickly in a sticky
available system. In the model we discussed, it is possible to achieve
several properties like monotonic reads in a highly available system
by waiting to reveal a write until all servers have seen it and its
relevant dependencies. However, this incurs severe visibility
penalties---new writes will not become visible to clients in the
presence of partitions. A client that does not want to guarantee
read-your-writes (due to the sticky availability requirement) may
still wish to read other clients' writes with timeliness.

