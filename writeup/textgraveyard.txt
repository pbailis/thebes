

(TODO: fix me--HotOS copy!)

Nonetheless, Coordinating transactional atomicity across replicas is
challenging. For example, if, as is common~\cite{readonly, eiger}, we
use a master to determine the correct set of updates to read, then the
system will not be available when clients are partitioned from the
master. Instead, we rely on decentralized \textit{background}
agreement to decide when to show new values, which can safely stall in
the presence of partitions. Each server keeps a known good value for
each data item $d$ it is assigned, $d_{good}$ and a set of pending
updates to $d$, $d_{pending}$. We use asynchronous agreement to move
updates from $d_{pending}$ to $d_{good}$: all of $d_{good}$'s
transactional siblings (or later values) are guaranteed present on all
replicas. Without aborts, this requires only one synchronous round
trip time (RTT).

\noindent\textit{Writes:} First, consider the case where transactions
do not internally abort. On commit, for each written data item $d$, we
send the last written value of $d$ ($d_v$) with the transaction ID and
$L_v$, a list of all data items and values written in the transaction,
to available replicas for $d$. The replicas immediately reply with a
response to \textit{commit}, place $d_v$ it in their respective
$d_{pending}$, and send a receipt to all replicas for items in
$L_v$. Once a server receives receipts from all replicas for all items
in $L_v$, the server sets $d_{good} = d_v$ if $d_v$'s ID is greater
than $d_{good}$'s ID. Accordingly, a new value will only appear in
$d_{good}$ when the other values written with $d_{good}$ ($L_v$) are
resident on every respective replica (though possibly in their
$d_{pending}$).\vspace{.5em}

\noindent\textit{Reads:} Each client maintains a vector of transaction
IDs $V$, with one entry per data item ($V(d)$). When a client reads a
new data item $d_i$, the server returns both $d_i$ and $L_v$, and, for
all data items $k \in L_v$, the client sets $V(k)$ to $d_i$'s ID. When
a client sends a read request for item $x$, it also sends $V(x)$, and
the replica returns \textit{either} $x_{good}$ if it matches $V(x)$ or
occurs after $V(x)$ in the transaction order or, if $x_{good}$ is
unsatisfactory, $V(x)$ from $x_{pending}$. If $V(x)$ is empty, the
server returns $x_{good}$.\vspace{.5em}

We can also consider arbitrary
application-defined partial orders, as in explicit
causality~\cite{explicit-socc}, if dependencies are specified at
commit time.

\noindent\textit{Aborts:} If we allow internal aborts, writes require
2 RTTs: one to check constraints and one to execute the above. Write
propagation and update visibility are achieved asynchronously, as in
the no-abort case.\vspace{.5em}

This implementation is entirely masterless and both reads and writes
do not block for coordination. One key property is that we do not, in general,
dictate when writes become visible (see: session guarantees,
\S\ref{sec:impossible}). Accordingly, it is highly available.\vspace{.5em}


The database tradition of providing ACID guarantees: atomicity,
consistency, isolation, and durability are, with few exceptions, not
addressed by the distributed systems literature. There are several
factors behind this occurrence, the least of which is that distributed
systems rarely consider multi-object guarantees, whereas ACID
transactions are frequently used to provide semantic guarantees across
multiple operations on multiple objects. While linearizability
(unfortunately often called ``atomicity'') ``composes'' across
multiple objects, a linearizable system does not provide ACID
``atomicity'' (which we will call ``transactional atomicity'' for
clarity): just because two writes are visible to all writers
immediately after they complete does not, on its own, provide any
guarantees about the mutual visibility of the two writes.

ecades of
research on semantics-based concurrency control and
coordination-reducing techniques such as escrow transactions and sagas
allow serializability in the presence of partial system knowledge or
otherwise reduce coordination cost. However, the majority of these
algorithms are intended to preserve application-level consistency,
which, for arbitrary applications and arbitrary sequence of
operations, cannot be ensured with high availability in the presence
of partitions.

 We might view the use of weak consistency is somewhat
of an ``arms race'': anecdotally, one major RDBMS vendor was forced to
lower their default isolation level in order to compare favorably in
out-of-the-box performance comparisons with its major competitor,
which had already lowered its default setting. Applications can guard
against inconsistency in a weakly isolated environment by performing
their own locking external to the database or by performing ad-hoc
concurrency control within the database. However, this appears
error-prone and, without further evidence, it remains somewhat
puzzling why this is indeed favorable to providing serializability
with in the database itself.

TRASH

We can accomplish these three requirements in two ways. First, clients
can locally cache their reads and writes, updating caches when they
read admissible replacements from
replicas~\cite{sessionguarantees}. Second, clients can ensure
\textit{stickiness} with groups~\cite{vogels-defs} (subject to the
same groups caveat as before).  For monotonic writes, clients should
generate increasing IDs for each transaction they perform. Caching and
ID generation are both available.

To provide writes follow reads,
severs (or clients) can buffer transaction writes until the
transactions the writes depend on have been written to all
replicas~\cite{cops}. If we consider a group model, each group can
independently apply transactions in order via a (possibly sharded)
log-shipping approach~\cite{causalmemory, cops, eiger, swift}.




What about existing systems?
* Database technology developed for single-node systems; gold standard: serializability
	* Serializability is not actually highly available; give an example!
	* Much of the database literature presumes serializability and does not consider high availability.
	* For the literature that doesn't presume serializability, it's not presented in a HA context?
* Consequence: traditional systems are not optimized for high availability
	* Consider two-phase locking in a distributed environment
	* As we will see (forward reference to Evaluation), many existing transactional systems encounter similar difficulties
* Key question in this paper: What transactional semantics are highly available, and which aren't?
	* Side note: there are infinite incomparable consistency models (e.g., always return 2, always return 3, â€¦)
	* Our goal is to unify distributed systems literature with ACID transaction model.
* Argument: Highly Available Transactional Systems (HATS) require rethinking existing models. This work is a first step.

In this sense, HATs are similar to RAID: optimizing for graceful
handling of a worst-case failure scenario improves average-case
performance.

IV. HATS and ACID (4pg)

We will show what semantics are available in HATS as well as which aren't. Our strategy will be to build up a set of properties until we reach a point of unavailability. We will start with isolation and atomicity since they are well-defined in the literature and most meaty.

A. *What's Available in Isolation and Atomicity?*

* We can prevent Dirty Write phenomena by consistently ordering transactions
	* This is basically a cross-data item convergency property, as are found in eventually consistent systems!
* Read Committed
	* Don't read dirty data by never exposing dirty data to readers!
* ANSI Repeatable Read => rename to cut isolation
	* Repeatable Read is a tricky guarantee. In ANSI SQL Spec, rather weak, but in follow-on work, it's pretty strong.
	* The property that the ANSI SQL spec talks about is about reading a "snapshot" of data items, along with your own writes.
		* A "snapshot" describes a "consistent cut" across data items; we'll call it "cut isolation"--when paired with below guarantees, makes sense.
	* This doesn't provide any constraints on writes.

* There are several properties that aren't discussed in the formal literature on isolation guarantees but which are really useful.

* Transactional atomicity: "A" in ACID
	* Once a transaction reads one of another transactions's writes, its subsequent reads will return the transactions's other writes (or a suitable "later" write).
	* Be really pedagogical here: "transactional atomicity" versus "distributed linearizability" both called "atomicity"; we actually explore the differences later, in Discussion section.
	* Talk about implementation here: it's basically uniform eager reliable delivery from distributed systems literature!
	* Note that this doesn't make recency guarantees (forward reference to later section on "impossibility")

* Session guarantees give useful guarantees within and across transactions
	* Without them, cut isolation is sort of meaningless: we can always just return NULL!
	* Several properties: monotonic writes, monotonic reads, writes follow reads, read your writes
	* We can define them within a transaction only (e.g., transactional monotonic writes) or across subsequent transactions by the same client (e.g., client monotonic writes).

* Session guarantees are tricky
	* You can implement all but read-your-writes in a R-HA system
		* Never show a write until all replicas in the system have seen it!
		* This means that, if a client switches replicas, then the replica will have a satisfying set of writes.
                * Take S3 as an example of a system where stickiness coudl have helped, but no one really talked about it!
		* Downside: when do writes become visible? Only when all replicas have seen it!
		* Side note: assuming that, if replicas can enter and leave, they can only become active once they "catch up" to global lower bound
			* We leave dynamic replica selection as Future Work (N.B. in APEs!)
	* Read-your-writes requires S-HA
		* Example: partition a client from all but one replica, have the client issue a write, which has to commit. Next, lift the partition between the client and the other replicas and partition the client and its previous replica. It can't read it's own write!
		* This also means that causality, which is MR+MW+WFR+RYW is unavailable!
		* S-HA is *doable* but it does result in substantially lowered unavailability unless one caches (e.g., bolt-on causal consistency)

\noindent\textit{Groups:} We can optimize write visibility delay by
partitioning servers and clients into groups, such that each group
contains a fully replicated set of data items and all clients within a
group contact only the servers in the group. Once all transaction
values are present within a group, values can be installed as
$d_{good}$ (as opposed to once they are present on all replicas). For
example, in a multi-datacenter setting, all clients and servers within
a datacenter may form a group. However, group-based atomicity is not
highly available according to our definition unless clients and
servers in a group fail together, as is often assumed in a
geo-replicated context~\cite{cops, eiger}.

* What do we have?
	* In Adya, we get up to Causal+Cut Isolation for S-HA, and PL-2 for R-HA.
	* In distributed systems terminology, in S-HA we have causal consistency but each transactions' updates are a cycle in the happens-before graph!
		* Stress that this is a big unification of the existing models.
	* Recap, pedagogically, why this is highly available and what this means: no locks, no central coordination, no need for RTTs; bring back to Section II.

B. *What's not available in Isolation?*

Now, at least in terms of the existing literature, we hit the limits.

Let's sketch out a few conditions, then we'll discuss them later:

Now refer to Adya's chart. No PL-SI, PL-SS, PL-2.99, etc.

C. *What's available in consistency?*

* In general, we can't maintain correctness conditions over arbitrary data items.
	* This is due to serializability problems.
	* Uniqueness, for example, is out the window

* We can evaluate locally checkable correctness criteria (e.g., check for null)
	(QUESTION: should we save this for APEs?)

* However, with semantic knowledge, we can do a little bit better
	* Commutative updates and logical monotonicity are fine; e.g., write-write conflicts don't matter
	
Example: In TPC-C New Order, the new order id assignment isn't commutative, tough. but the inventory checking *is*!

D. Durability

* If you want data to survive *F* failures, you'll have to replicate to *F* replicas.
	* This is pretty easy.

E. Summary

* Left with a bunch of binary properties: 
R-HA: Prevent Dirty Write, Read Committed, Cut Isolation, Transactional Atomicity, Transactional Monotonic Reads, Transactional Monotonic Writes, Transactional Writes-Follow-Reads, Client Monotonic Reads, Client Monotonic Writes, Client Writes-Follow-Reads
S-HA: Client Read Your Writes, Transactional Read Your Writes

* Combining all of the above results in the strongest models we've yet seen. No

*Discussion*
