
\section{High Availability}
\label{sec:availability}

To understand which guarantees can be provided with high availabilty,
we must first define what high availability means. In this section, we
will formulate a model that captures a range of availability models,
including ``true'' high availability, availablility with stickiness,
and several other useful variants.

Informally, highly available algorithms ensure ``always on'' operation
and guaranteed low latency. If users of a highly available system are
able to contact a (set of) server(s) in a system, they are guaranteed
a response; this means servers will not need to communicate with any
others. If servers are partitioned from one another, they do not need
to stall in order to provide a ``safe'' query response. This lack of
fast-path coordination also means that a highly available system also
provides low latency~\cite{abadi-pacelc}; in a wide-area setting, this
means that clients need not need to wait for cross-datacenter
communication. However, to properly describe whether a
\textit{transactional} system is highly available, we need both a way
to describe what servers a client must contact as well as what kinds
of responses a server can provide, especially given the possibility of
aborts.

\subsection{Replica Availability}

Traditionally, a system provides high availability if every user that
can contact a server eventually receives a response from that server,
even in the presence of arbitrary, indefinitely long network
partitions between servers~\cite{gilbert-cap}. As in a standard
distributed database, designated servers might handle operations on
different data items;\footnote{There is a further distinction between
  a \textit{fully replicated} system, in which all servers are
  replicas for all data items, and a \textit{partially replicated}
  system, in which at least one server acts as a replica for only a
  subset of all data items. For generality, and, for applicability to
  many modern ``sharded'' or ``partitioned'' data storage systems like
  Dynamo, we consider partially replicated systems.} a server that can
handle an operation for a given data item is called a \textit{replica}
for that item. While this standard model is useful, there are several
other cases we should consider.

First, clients may wish to contact multiple servers. For example, to
ensure that the effects of operations will persist in the event of
server failures, clients may wish to wait until operations reach more
than one server before they consider the operations
completed. However, according to our definition of high availability,
server-fault tolerance is unavailable. To allow flexibility in the
number of participating servers required for an operation, we propose
\textit{$K$-availability}: if a client can contact $K$ servers, then
it eventually receives a response from all of them, even in the
presence of indefinitely long network partitions. Compared to a system
providing high availability ($1$-availability), a system with
$2$-availability will require more messaging and possibly higher
latency and may experience more unsuccessful operations in the
presence of network and server failures.  As is commmon in standard
consensus algorithms like Paxos, clients may want to contact a
majority of $N$ servers in a system. This \textit{majority
  availability} (a special case of $K$-availability with $K=\lceil
\frac{N}{2} \rceil$) is ``more available'' than full $N$-availability.

Clients may also wish to contact the same server (or set of servers)
across subsequent operations. As we will discuss in
Section~\ref{sec:hats}, clients can ensure continuity between
operations (e.g., reading their prior updates to a data item) by
maintaining affinity or ``stickiness'' with a server or set of
servers~\cite{vogels-defs}. Simultaneously maintaining availability
and stickiness entails fate-sharing between the client and its set of
servers: if a client's sticky servers fail, or, equivalently, if the
client is partitioned from its sticky servers, then it may face
unavailability. We say that a system provides \textit{sticky
  availability} if, whenever a client's operation is executed against
a server that has observed all of its prior operations, it eventually
receives a response, even in the presence of indefinitely long
partitions. As above, it may be useful for a client to achieve
stickiness \textit{and} contact many replicas (i.e., for fault
tolerance), so we generalize sticky availability to \textit{$K$-sticky
  availability}, which provides sticky availability with $K$ distinct
servers. A client may choose to become $1$-sticky available by acting
as a server itself; for example, a client might cache its reads and
writes. However, a single client cannot achieve $K$-sticky
availability with $K>1$ without contacting other replicas. Moreover, ``sticking'' all clients
to a designated set of servers (i.e., master availability) is a
special case of sticky availability and is ``less available'' than the
equivalent non-mastered sticky availability.

\begin{figure}
\centering
\begin{tikzpicture}[scale=0.8]
  \tikzstyle{every node}=[font=\small]
 \node[draw=none,fill=none] (1) at (0,0) {$1$-availability (high availability)}; 
 \node[draw=none,fill=none] (2) at (0,1) {$2$-availability}; 
 \node[draw=none,fill=none] (m-1) at (0, 2) {\ldots}; 
 \node[draw=none,fill=none] (m) at (0, 3) {majority availability}; 
 \node[draw=none,fill=none] (m+1) at (0, 4) {\ldots}; 
 \node[draw=none,fill=none] (n) at (0, 5) {$N$-availability}; 
 \node[draw=none,fill=none] (1s) at (4, 1.5) {$1$-sticky availability};
 \node[draw=none,fill=none] (2s) at (4, 2.5) {$2$-sticky availability};
 \node[draw=none,fill=none] (n2s) at (4, 3.5) {\ldots};
 \node[draw=none,fill=none] (n1s) at (4, 4.5) {$(N$$-$$1)$-sticky availability};

 \draw [->] (1) -- (2);
 \draw [->] (2) -- (m-1);
 \draw [->] (m-1) -- (m);
 \draw [->] (m) -- (m+1);
 \draw [->] (m+1) -- (n);
 \draw [->] (1) -- (1s);
 \draw [->] (1s) -- (m);
 \draw [->] (2s) -- (m+1);
 \draw [->] (1s) -- (2s);
 \draw [->] (2) -- (2s);
 \draw [->] (2s) -- (n2s);
 \draw [->] (n2s) -- (n1s);
 \draw [->] (n1s) -- (n);


\end{tikzpicture}
\caption{Hierarchy of replica availability levels for $N>3$ servers.}
\label{fig:availability-order}
\end{figure}

We show a hierarchy of replica availability levels in
Figure~\ref{fig:availability-order}. $K$-sticky availability subsumes
$K$-availability, but $K$-sticky availability is incomparable with
$(K+1)$-availability. $N$-sticky availability is equivalent to
$N$-availability, while majority availability subsumes $1$-sticky
availability. More generally, $\lceil \frac{N}{2} \rceil$$+$$K$$-$$1$
availability subsumes $K$-sticky availability. We have omitted
discussion of operation-specific availability levels (e.g.,
$N$-availability for writes and $1$-availability for reads in a
write-all, read-one data store), system membership changes, or
heterogeneous replicas (e.g., servers in a local and remote
datacenters) but believe there are several avenues for further
taxnomization.

\subsection{Transactional Availability}

So far, we have focused on single-object, single-operation
availability. This is standard in distributed systems literature
(e.g., linearizable, regular, and safe register models all concern
single objects~\cite{herlihy-art}), yet the database literature
largely focuses on transactions: groups of multiple operations over
multiple objects. Accordingly, by itself, replica availability is
insufficient to describe availability guarantees for
transactions. Additionally, given the choice of \textit{commit} and
\textit{abort} responses---which signal transaction success or failure
to a client---we must be careful in how we define transactional availability.

If a client wishes to execute a transaction that performs operations
on multiple data items, then the client must be able to contact and
receive a response from at least one replica for each data item. This
may result in ``lower availability'' than a non-transactional
requirement. Additionally, given the possibility of aborts, we need to
ensure that the system makes useful forward progress: a system can
trivially guarantee clients a response by always aborting all
transactions~\cite{transaction-liveness}. However, this is an
unsatisfactory system because nothing good (transaction commit) ever
happens; we should require a \textit{liveness} property. We
cannot guarantee that every transaction will commit---transactions may
choose to abort themselves---but we need to make sure that the system
will not indefinitely abort transactions on its own volition. We call
a transaction abort due to a transaction's own choosing (e.g., as an
operation of the transaction itself or due to a would-be declared
integrity constraint violation) an \textit{internal abort} and an
abort due to system implementation an \textit{external abort}.

We say that a system provides \textit{transactional availability} if,
given replica availability for every data item in a transaction, the
transaction eventually commits or internally
aborts~\cite{hat-hotos}. For example, a system will violate
transactional $2$-availability if a client that can contact two
servers for each of the data items in its transaction does not
eventually commit in the absence of internal aborts.

