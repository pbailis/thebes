
\section{High Availability}
\label{sec:availability}

To understand which guarantees can be provided with high availabilty,
we must first define what high availability means. In this section, we
will formulate a model that captures a range of availability models,
including ``true'' high availability, availability with stickiness,
and several other useful variants.

Informally, highly available algorithms ensure ``always on'' operation
and guaranteed low latency. If users of a highly available system are
able to contact a (set of) server(s) in a system, they are guaranteed
a response; this means servers will not need to communicate with any
others. If servers are partitioned from one another, they do not need
to stall in order to provide a ``safe'' query response. This lack of
fast-path coordination also means that a highly available system also
provides low latency~\cite{abadi-pacelc}; in a wide-area setting, this
means that clients need not wait for cross-datacenter
communication. To properly describe whether a \textit{transactional}
system is highly available, we need to describe what servers a client
must contact as well as what kinds of responses a server can provide,
especially given the possibility of aborts.

\subsection{Replica Availability}

Traditionally, a system provides {\em availability} if every user that
can contact a correct server eventually receives a response from that
server, even in the presence of arbitrary, indefinitely long network
partitions between servers~\cite{gilbert-cap}.\footnote{Under this
  definition, a system that requires a majority of servers to be
  online is not availabile.} As in a standard distributed database,
designated servers might handle operations on different data
items;\footnote{There is a further distinction between a \textit{fully
    replicated} system, in which all servers are replicas for all data
  items, and a \textit{partially replicated} system, in which at least
  one server acts as a replica for only a subset of all data
  items. For generality, and, for applicability to many modern
  ``sharded'' or ``partitioned'' data storage systems~\cite{dynamo,
    pnuts, bigtable, spanner, hstore}, we consider partially
  replicated systems.} a server that can handle an operation for a
given data item is called a \textit{replica} for that item. While this
standard model is useful, there are several other cases we should
consider.

Some algorithms assume a model in which the client always contacts the
same server across subsequent operations. As we will discuss in
Section~\ref{sec:hats}, clients can ensure continuity between
operations (e.g., reading their prior updates to a data item) by
maintaining affinity or ``stickiness'' with a server or set of
servers~\cite{vogels-defs}. Simultaneously maintaining availability
and stickiness entails fate-sharing between the client and its 
server: if a client's sticky server fail, or, equivalently, if the
client is partitioned from its sticky server, then it may face
unavailability. We say that a system provides \textit{sticky
  availability} if, whenever a client's operation is executed against
a correct server that has observed all of its prior operations, it eventually
receives a response, even in the presence of indefinitely long
partitions. A client may choose to become sticky available by acting
as a server itself; for example, a client might cache its reads and
writes~\cite{bolton, sessionguarantees, swift}. 

Availability implies sticky availability, since availability does not
require that the contacted server has seen all the previous operations
of a client, whereas sticky availability is the special case in which
the system only guarantees correctness if the contacted server has
seen all the prior operations from the client.

%% \begin{figure}
%% \centering
%% \begin{tikzpicture}[scale=0.8]
%%   \tikzstyle{every node}=[font=\small]
%%  \node[draw=none,fill=none] (1) at (0,0) {$1$-availability (high availability)}; 
%%  \node[draw=none,fill=none] (2) at (0,1) {$2$-availability}; 
%%  \node[draw=none,fill=none] (m-1) at (0, 2) {\ldots}; 
%%  \node[draw=none,fill=none] (m) at (0, 3) {majority availability}; 
%%  \node[draw=none,fill=none] (m+1) at (0, 4) {\ldots}; 
%%  \node[draw=none,fill=none] (n) at (0, 5) {$N$-availability}; 
%%  \node[draw=none,fill=none] (1s) at (4, 1.5) {$1$-sticky availability};
%%  \node[draw=none,fill=none] (2s) at (4, 2.5) {$2$-sticky availability};
%%  \node[draw=none,fill=none] (n2s) at (4, 3.5) {\ldots};
%%  \node[draw=none,fill=none] (n1s) at (4, 4.5) {$(N$$-$$1)$-sticky availability};

%%  \draw [->] (1) -- (2);
%%  \draw [->] (2) -- (m-1);
%%  \draw [->] (m-1) -- (m);
%%  \draw [->] (m) -- (m+1);
%%  \draw [->] (m+1) -- (n);
%%  \draw [->] (1) -- (1s);
%%  \draw [->] (1s) -- (m);
%%  \draw [->] (2s) -- (m+1);
%%  \draw [->] (1s) -- (2s);
%%  \draw [->] (2) -- (2s);
%%  \draw [->] (2s) -- (n2s);
%%  \draw [->] (n2s) -- (n1s);
%%  \draw [->] (n1s) -- (n);


%% \end{tikzpicture}
%% \caption{Hierarchy of replica availability levels for $N>3$ servers.}
%% \label{fig:availability-order}
%% \end{figure}

%% We show a hierarchy of replica availability levels in
%% Figure~\ref{fig:availability-order}. $K$-sticky availability subsumes
%% $K$-availability, but $K$-sticky availability is incomparable with
%% $(K+1)$-availability. $N$-sticky availability is equivalent to
%% $N$-availability, while majority availability subsumes $1$-sticky
%% availability. More generally, $\lceil \frac{N}{2} \rceil$$+$$K$$-$$1$
%% availability subsumes $K$-sticky availability. We have omitted
%% discussion of operation-specific availability levels (e.g.,
%% $N$-availability for writes and $1$-availability for reads in a
%% write-all, read-one data store), system membership changes, or
%% heterogeneous replicas (e.g., servers in a local and remote
%% datacenters) but believe there are several avenues for further
%% taxonomization.

\subsection{Transactional Availability}

We have so far focused on single-object, single-operation
availability. This is standard in the distributed systems literature
(e.g., linearizable and regular register models all concern single
objects~\cite{herlihy-art}), yet the database literature largely
focuses on transactions: groups of multiple operations over multiple
objects. Accordingly, by itself, replica availability is insufficient
to describe availability guarantees for transactions. Additionally,
given the choice of \textit{commit} and \textit{abort}
responses---which signal transaction success or failure to a
client---we must carefully define transactional availability.

If a client wishes to execute a transaction that performs operations
on multiple data items, then the client must be able to contact and
receive a response from at least one replica for each data item. This
may result in ``lower availability'' than a non-transactional
requirement. Additionally, given the possibility of aborts, we need to
ensure useful forward progress: a system can trivially guarantee
clients a response by always aborting all
transactions~\cite{transaction-liveness}. However, this is an
unsatisfactory system because nothing good (transaction commit) ever
happens; we should require a \textit{liveness} property. We cannot
guarantee that every transaction will commit---transactions may choose
to abort themselves---but we need to make sure that the system will
not indefinitely abort transactions on its own volition. We call a
transaction abort due to a transaction's own choosing (e.g., as an
operation of the transaction itself or due to a would-be declared
integrity constraint violation) an \textit{internal abort} and an
abort due to system implementation an \textit{external abort}.

We say that a system provides \textit{transactional availability} if,
given replica availability for every data item in a transaction, the
transaction eventually commits or internally
aborts~\cite{hat-hotos}. For example, a system will violate
transactional availability if a client that can contact two
servers for each of the data items in its transaction does not
eventually commit in the absence of internal aborts.

