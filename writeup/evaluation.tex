
\section{HAT Implications}
\label{sec:evaluation}

With an understanding of which guarantees are and are not
HAT-compliant, in this section, we analyze the implications of these
results for existing systems and applications and briefly study HAT
systems on public cloud infrastructure. Specifically:

\begin{myenumerate}
\item We revisit traditional database concurrency control with a focus
  on coordination costs and on high availability.
\item We examine the properties required by a realistic OLTP
  application patterned on the TPC-C benchmark.
\item We perform a brief experimental evaluation of HAT versus non-HAT
  properties on public cloud infrastructure.
\end{myenumerate}

\subsection{Existing Algorithms}

Many existing database transaction and concurrency control algorithms
are not designed for high availability. These algorithms often presume
a single-server deployment or the requirement for serializability. As
a consequence, traditional transaction processing systems are not
well-optimized for a HAT context. In this section, we briefly discuss
design decisions and algorithmic details that preclude high
availability.

\vspace{.5em}\noindent\textbf{Serializability} To establish a serial
order on transactions, algorithms for achieving serializability of
general-purpose read-write transactions in a distributed
setting~\cite{bernstein-book} may require at least one round trip time
(RTT) before committing. As an example, traditional two-phase locking
for a transaction of length $T$ requires $T$ \texttt{lock} operations
and at least one \texttt{unlock} operation.  In a distributed
environment, each of these operations requires coordination, either
with other database replicas or with a lock service. If this
coordination mechanism is unavailable, transactions cannot safely
commit. Similarly, optimistic concurrency control requires
coordinating via a validation step, while deterministic transaction
scheduling~\cite{deterministic-scheduling} requires a
scheduler. Serializability under multi-version concurrency control
also requires checking for update conflicts. The reliance on a
globally agreed total order necessitates a minimum of one round-trip
to a designated master or coordination service for each of these
classic algorithms.  The cost of the (minimum one) round trip will be
determined by the deployment environment, as we saw in
Section~\ref{sec:motivation}; we will demonstrate this cost on public
cloud infrastructure in the next section.

\vspace{.5em}\noindent\textbf{Non-serializability} Many existing
distributed implementations of weak isolation are not highly
available. Lock-based proposals such as those used to provide weak
isolation in Gray's original proposal~\cite{gray-isolation} do not
degrade gracefully in the presence of partial failure. (Note, however,
that lock-based protocols \textit{do} offer the benefit of recency
guarantees.) While multi-versioned storage systems allow for a variety
of transactional guarantees, we have not seen proposals for truly weak
isolation (e.g., non-``tentative update'' schemes) in this context.
The MDCC~\cite{mdcc} protocol offers Read Committed isolation with
Lost Update avoidance but is similarly unavailable due to its reliance
on preventing write conflicts. Chan and Gray's read-only transactions
provide read-only transactions with item-cut isolation, causal
consistency, and transactional atomicity (session PL-2L~\cite{adya})
but are unavailable in the presence of coordinator
failure~\cite{readonly}, similar to read-only and write-only
transactions more recently proposed by Eiger~\cite{eiger}; Causal
Serializability offers a similar model (with unavailable
implementation): causal consistency with a variant of Read Uncommitted
between transactions that write to the same data
item~\cite{raynal-causal}.  Bolt-on causal consistency~\cite{bolton},
Brantner's S3 database~\cite{kraska-s3}, and
Bayou~\cite{sessionguarantees} can all provide variants of session
PL-2L with high availability, but none provide this HAT functionality
without substantial modification; Swift~\cite{swift} is closest to
providing full HAT operation in a sticky context. As we have seen, it
is possible to implement many guarantees weaker than
serializability---including guarantees that are achievable with high
availability---and still not achieve high availability.

\subsection{Application Requirements}

Thus far, we have largely ignored the question of which applications
require semantics that are unavailable to HATs. As we showed in
Section~\ref{sec:hats}, the main cost of choosing HATs comes in the
inability to prevent Lost Update, Write Skew, and provide recency
bounds. In this section, we attempt to understand when these
guarantees matter both abstractly and in a representative
transactional application based on the TPC-C benchmark~\cite{tpcc}.

\vspace{.5em}\noindent\textbf{Commutativity and Monotonicity} Recent
work on the CALM Theorem~\cite{calm}, Lattice-based state
update~\cite{blooml}, and Commutative and Replicated Data
Types~\cite{crdt} demonstrates that, if updates logically commute,
then they can often be safely performed in different orders at
different replicas. Accordingly, as long as all writes are delivered
to all replicas, then a system executing monotonic logic with
commutative operators may not suffer from application-level
consistency anomalies as a result of Lost Update or Write Skew
anomalies. However, applications with non-monotonic state mutation
will not, in general, be able to maintain application-level
consistency constraints with HATs alone. Moreover, applications with
requiring bounded update visibility latency should opt for
unavailability.

\vspace{.5em}\noindent\textbf{TPC-C} To better understand the impact
of HAT-compliance in an application context, we consider a concrete
application: the TPC-C benchmark. Surprisingly, four of five
transactions can be executed with HATs, while the fifth may require
unavailablity.

TPC-C consists of five transactions, capturing the operation of a
wholesale warehouse, including sales, payments, and deliveries. Two
transactions---\textit{Order-Status} and \textit{Stock-Level}---are
read-only and can be executed safely under HATs. Clients may read
stale data, but this does not violate TPC-C requirements and clients
will read their writes if they are sticky-available. Another
transaction type, \textit{Payment}, updates running balances for
warehouses, districts, and customer records as well as providing an
audit trail. The transaction is increment- and append-only, so all
balance increase operations commute, and TA allows the
maintainence of any foreign-key constraints (e.g.,
\texttt{UPDATE/DELETE CASCADE}).

While three out of five transactions are easily achievable with
HATs, the remaining two transactions---\textit{New-Order} and
\textit{Delivery}---are not as simple. The New-Order transaction
places an order for a variable quantity of data items, updating
warehouse stock as needed. It selects a sales district, assigns the
order an ID number, adjusts the remaining warehouse stock, and writes
a placeholder entry for the pending order. The Delivery transaction
represents the fulfillment of a New-Order: it deletes the order from
the pending list, updates the customer's balance, updates the order's
carrier ID and delivery time, and updates the customer balance.

New-Order presents two challenges: ID assignment and stock
maintainence. First, each New-Order transaction requires a unique ID
number for the order. We can create a unique number by, say,
concatenating the client ID and a timestamp. However, the TPC-C
specification requires order numbers to be \textit{sequentially}
assigned within a district, which requires preventing Lost
Update. Accordingly, HATs cannot provide compliant TPC-C execution but
can maintain uniqueness constraints. Second, the New-Order transaction
decrements inventory counts: what if the count becomes negative?
Fortunately, TPC-C New-Order restocks each item's inventory count
(increments by 91) if it would become negative as the result of
placing an order. This means that, even in the presence of concurrent
New-Orders, an item's stock will never fall below zero. This is TPC-C
compliant, but a HAT system might end up with substantially more stock
than in a non-HAT-compliant implementation.

Delivery is challenging due to non-monotonicity. Each Delivery deletes
a pending order from the NewOrder Table and should be idempotent in
order to avoid billing a customer twice; this implies a need to
prevent Lost Update. We can avoid this issue by moving the
non-monotonicity to the real world---the carrier that picks up the
package for an order can ensure that she is the only carrier who has
done so---but cannot provide a correct execution with HATs
alone. However, according to distributed transaction
architects~\cite{entitygroup}, these compensatory actions are
relatively common in real-world business processes.

Throughout execution, TPC-C also requires the maintainence of several
integrity constraints. For example, Consistency Condition 1 (3.3.2.1)
requires that each warehouse's sales count must reflect the sum of its
subordinate sales districts. This integrity constraint spans two
tables but, given the ability to update rows in both tables atomically
via transactional atomicity, can be easily maintained. Consistency
Conditions 4 through 12 (3.3.2.4-12) can similarly be satisfied by
applying updates atomically across tables. Consistency Conditions 2
and 3 (3.3.2.2-3) concern order ID assignment and are more
problematic, as we have discussed.  Finally, while TPC-C is not
subject to multi-key anomalies, we note that many TPC-E isolation
tests, such as one that requires simultaneously modifying a product
description and its image, are also passable using HATs.

In summary, many---but not all---TPC-C transactions are well-served by
HATs. The two problematic transactions---New-Order and Payment---rely
on non-monotonic state update. The former transaction can be modified
to ensure ID uniqueness at the expense of sequential per-district ID
ordering, while the latter is an inherently non-monotonic action
requiring external compensation or stronger consistency protocols. We
expect that, especially for read-dominated workloads found in many
online services, HAT guarantees will suffice for many application
transactions.

\subsection{Experimental Costs}

\begin{figure}
\includegraphics[width=.8\columnwidth]{figs/macrotracelegend.pdf}\vspace{-2mm}
\includegraphics[width=\columnwidth]{figs/lat-thru-trace-K0-SCALEMACRO.pdf}
\includegraphics[width=\columnwidth]{figs/lat-thru-trace-K0-SCALEMACRO.pdf}
\caption{Latency-throughput for database operation on EC2 (DUMMY FIG)}
\label{fig:wan-exp}
\end{figure}

2.) We ran YCSB on EC2 (and maybe TPC-C if we have time) with a few models:
	- Eventual Consistency
	- Eventual consistency with a "master" for updates--simulates the lower bound on a non-HAT system.
	- Transactional Atomicity and Read Committed
	
	-This is going to look a lot like the ping tests: much higher latency for "non-HATs."
	-Our Naive 2PL implementation bottlenecks extremely quickly.
	-On, say, 5 servers, can get hundreds of thousands of TPS

	-Not meant as an exhaustive study, but validates our intuitions. Coordination costs of non-HAT systems are unaccounted for, will only go up, and we haven't even talked about availability.
