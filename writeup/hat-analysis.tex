
\section{Highly Available Transactions}
\label{sec:hats}

HAT systems provide transactions with transactional availability and
either high availability or sticky high availability. They offer
latency and availability benefits over traditional distributed
databases, yet they cannot achieve all possible semantics. In this
section, we delineate ACID, distributed consistency, and session
consistency levels which can be achieved with high availability
(transactional atomicity, variants of Repeatable Read isolation, and
many session guarantees), those with sticky high availability (read
your writes, PRAM and causal consistency), and properties that cannot
be provided in a HAT system (those preventing Lost Update and Write
Skew, or with recency).  We present a full summary of these results in
Section~\ref{sec:hat-summary}.

As Brewer states, ``systems and database communities are separate but
overlapping (with distinct vocabulary)''~\cite{brewer-slides}. With
this challenge in mind, when possible, we build on existing properties
and definitions from the database and distributed systems literature,
providing a brief, informal explanation and example for each
guarantee. The database isolation guarantees require particular care,
since different DBMSs often use the same terminology for different
mechanisms and may provide additional guarantees in addition to our
implementation-agnostic definitions.  We draw largely on Atul Adya's
dissertation~\cite{adya} and somewhat on its predecessor work: the
ANSI SQL specification~\cite{ansi-sql} and Berenson et al.'s
subsequent 1995 critique~\cite{ansicritique}. We provide a set of
formal definitions, semantics, proofs and observations in our extended
Technical Report~\cite{hat-tr} and opt for a more informal
presentation here.

\subsection{Achievable HAT Semantics}

To begin, we present well-known semantics that can be achieved in HAT
systems. In this section, our goal is feasibility, not performance. As
a result, we offer proof-of-concept highly available algorithms, but
our goal is \textit{not} to provide optimal or even efficient
implementations: the challenge is to provide algorithms that provide
high availability. However, we briefly study performance implications
in Section~\ref{sec:evaluation}.

In our examples, we exclusively consider read and write operations,
denoting a write of value $v$ to data item $d$ as $w_d(v)$ and a read
from data item $d$ returning $v$ as $r_d(v)$. We assume that all data
items have the null value, $\bot$, at database initialization, and,
unless otherwise specified, all transactions in the examples commit.

\subsubsection{ACID Isolation Guarantees}

To begin, \textbf{Read Uncommitted} isolation is captured by Adya as
\textit{PL-1}, requiring only that each transaction's writes are ordered
consistently according to a single total order on transactions. This
prohibits Adya's phenomenon $G0$, also called ``Dirty
Writes''~\cite{adya}. More formally, we require a total ordering on
writes to each data item and we create a graph of transactions with
edges from one transaction to another if the former overwrites the
latter's write to the same object. \textit{PL-1} prohibits cycles in
this graph, requiring some total order on writes. Accordingly, the
final database state cannot contain the ``earlier'' transaction's
writes to the data items: for example, in the below example, $T_3$
should \emph{eventually} only read $a=b=1$ or $a=b=2$ but not $a=2,
b=1$ or $a=1, b=2$:
\begin{align*}
\small\vspace{-.5em}
T_1 &: w_x(1)~w_y(1)
\\T_2 &: w_x(2)~w_y(2)
\\T_3 &: r_x(a)~r_y(b)\vspace{-.5em}
\end{align*}
Later properties will strengthen Read Uncommitted.

Unlike the total order required by serializability, the order in PL-1
does not constrain the values seen by a transaction's read
operations except once the database has reached a ``final
state.'' Read Uncommitted is easily achieved via applying the same
logical timestamp to each update in a transaction and applying a
``last writer wins'' conflict reconciliation policy at each replica.

\textbf{Read Committed} isolation is particularly important in
practice as it is the default of many DBMSs. Centralized
implementations differ, with some based on long-duration exclusive
locks and short-duration read locks~\cite{gray-isolation} and others
based on multiple versions. The implementations often provide recency
and monotonicity properties beyond the simple meaning from the name,
which is what is expressed in the implementation-agnostic definition:
under Read Committed, transactions should not access uncommitted or
intermediate versions of data items. This prohibits both ``Dirty
Writes'', as above, and also ``Dirty Reads'' phenomena.  This
isolation is Adya's \textit{PL-2} and is formalized by prohibiting
Adya's \textit{G1\{a-c\}} (or ANSI's $P1$, or ``broad'' $P1$ [2.2]
from Berenson et al.). For instance, in the example below, $T_3$
should never see $a=1$, and, if $T_2$ aborts, $T_3$ should never read
$a=3$:
\begin{align*}
\small\vspace{-.5em}
T_1 &: w_x(1)~w_x(2)
\\T_2 &: w_x(3)\\
T_3 &: r_x(a)\vspace{-.5em}
\end{align*}
It is fairly easy for a HAT system to prevent ``Dirty Reads'': if each
client never writes uncommitted data to shared copies of the database, then
transactions will never read each others' dirty data. As a simple
solution, clients can buffer their writes until they commit, or,
alternatively, can send them to servers, who will not deliver their
value to other readers until notified that the writes have been
committed. This implementation does not provide recency or
monotonicity guarantees but satisfies the implementation-agnostic
definition.

\textbf{Repeatable Read} isolation is a confusing property. For
example, some IBM products use the term for fully serializable
isolation~\cite{hat-hotos}, while Gray~\cite{gray-isolation}, Berenson
et al.~\cite{ansicritique}, and Adya's PL-2.99~\cite{adya} all
interpret Repeatable Read as providing serializability for all
operations except for predicate-based reads (and so Repeatable Read is
identical to serializability in a key-value store without
predicate-based access). In models like Adya's PL-2.99, the
\textit{Phantom Problem}, whereby two successive predicate-based reads
return different data~\cite{gray-isolation}, is the only kind of
non-serializable behavior allowed. In this paper, we reserve the use
of ``Repeatable Read'' to refer to these definitions, which we will
soon show are not HAT-compliant.

However, as Berenson et al.~\cite{ansicritique} discuss, the ANSI SQL
specification allows several additional behaviors under ``Repeatable
Read'' isolation.  \textbf{ANSI Repeatable Read} requires that, along
with respecting Read Committed isolation, each transaction will only
read one version of each data item that it did not itself produce
(preventing ``Fuzzy Read,'' or P2). In the example below, $T_3$ must
read $a=1$:
\begin{align*}
\small
T_1 &: w_x(1)
\\T_2 &: w_x(2)
\\T_3 &: r_x(1)~r_x(a)
\end{align*}
This is a literal interpretation of the phrase ``Repeatable Read'':
unless a transaction modifies a given data item, the observable value
of the data item should not change during the transaction. By itself,
this property is weak---we can always read $\bot$ for each data
item---and easily achieved in a HAT system by caching the values
read. To prevent confusion, we call this isolation model \textbf{Item
  Cut Isolation}: all the transaction's reads should see values from a
non-changing consistent cut or snapshot over the data items. It is
possible to satisfy Item Cut Isolation with high availability by using
multi-versioning on each replica and ensuring that each of a
transaction's reads return an appropriately chosen version (e.g.,
identified by vector clock).

A stronger achievable property that expands on Item Cut Isolation
requires consistent cuts to span both a transaction's predicate reads
(e.g., \texttt{SELECT WHERE}) as well as its item reads.  We call this
\textbf{Predicate Cut Isolation} and it prohibits Phantoms (but does
not guarantee serializability of item-based reads and
writes). Predicate Cut Isolation is also achievable in HAT systems via
similar multi-versioning as above.

\subsubsection{ACID Atomicity Guarantees}

\textbf{Transactional atomicity (TA)} is core to ACID guarantees. Although, at
least by the ACID acronym, it is not an ``isolation'' property, TA
restricts transactions' ability to view the effects of partially
completed transactions. Under TA, once some of the effects of a
transaction $T_i$ are observed by another transaction $T_j$,
thereafter all effects of $T_i$ are observed by $T_2$. Together with
item cut isolation, TA prevents Read Skew anomalies (Berenson et al.'s
A5A~\cite{ansicritique}). As an example of TA, because $T_2$ has read
$T_1$'s write to $y$, $T_2$ must observe $b=c=1$ (or later versions
for each key):
\vspace{-.5em}
\begin{align*}
\small
T_1 &: w_x(1)~w_y(1)~w_z(1)
\\T_2 &: r_x(a)~r_y(1)~r_x(b)~r_z(c)~\\[-1.5em]
\end{align*}
$T_2$ can also observe $a=\bot$, $a=1$, or a later version of
$a$. Notably, TA requires Read Committed isolation: observing all
effects of a transaction implicitly requires observing the final
(committed) effects of a transaction as well.

Perplexingly, discussions of TA are absent from existing treatment of
weak isolation. This is perhaps again due to the single-node context
in which prior work was developed: on a single server (or a fully
replicated database), TA is achievable via lightweight locking and/or
local concurrency control over data items~\cite{gstore}. In contrast,
in a distributed environment, TA over arbitrary groups of
non-colocated items is considerably more difficult to achieve with
high availability: servers need to ensure that, regardless of replica
selection, all of a client's accesses will read a sufficiently
up-to-date/``consistent'' set of data items. However, replicas do not
need to agree on when to choose to reveal new sets of values clients,
which would require consensus. Instead, TA only requires the
equivalent of reliable broadcast (with some additional client
metadata) and is achievable in HAT systems.

TA can be achieved via multi-versioning. If clients attach vector
clocks to every write (incrementing their own position for each
transaction) and replicas store all writes via multi-versioning, then
clients can safely determine which sets of data items to read; this
approach is adopted by Swift~\cite{swift} and, to a lesser-extent,
bolt-on causal consistency~\cite{bolton}. These systems also provide
causal consistency, which requires sticky availability (in each,
implemented via client-side caching). However, TA is achievable
without stickiness via an alternate algorithm: if servers wait to
reveal writes until they are present on all replicas, clients do not
need to be sticky.

We sketch a non-sticky TA algorithm and provide greater detail in our
technical report~\cite{hat-tr}. Each replica waits to reveal write $w$
from transaction $T$ until all of the writes in $T$ are present on all
respective replicas (are \textit{pending stable}). Once they are,
replicas asynchronously reveal their writes. To prevent clients from
reading writes that have been revealed on some servers but not others,
clients can attach a vector that uniquely identifies the transaction:
this can be the transaction ID and a list of keys in the
transaction. When clients attempt to read a data item from a replica,
they can send the highest ID they have observed for that item;
replicas respond with either a matching write (which may or may not be
pending stable) or a write with higher ID that is pending
stable. Clients can reset their record of IDs and keys upon
transaction commit. Servers keep two sets of versions for each data
item: the value with the highest ID that is pending stable and a set
of values that are not yet pending stable.

This non-sticky implementation is entirely masterless and neither reads
nor writes block for coordination. Moreover, there are several
possible optimizations that, for brevity, we do not describe
here. However, one key property is that we do not, in general, dictate
when writes become visible. Accordingly, it is highly
available. However, we can optimize write visibility delay by ensuring
that clients are sticky with disjoint groups of servers. Once all
transaction values are \texttt{pending} within a group of servers,
values can be made \texttt{stable} (as opposed to once they are
present on all servers). For example, in a multi-datacenter setting,
all clients and servers within a datacenter may form a sticky group.


\subsubsection{Session Guarantees}

As is typical in the database literature (with few
exceptions~\cite{daudjee-session}), our models have not yet considered
interactions \textit{across} transactions, other than that there
exists some arbitrary ordering on them (via Read Uncommitted). In the
distributed systems literature, many useful \textit{safety} guarantees
span multiple, non-transactional operations. In particular,
\textit{session guarantees} are used to describe guarantees across
transactions within a given \textit{session}, ``an abstraction for the
sequence of...operations performed during the execution of an
application''~\cite{sessionguarantees}. Informally a session describes
a context that should persist between transactions: for example, on a
social networking site, all of a user's transactions submitted between
``log in'' and ``log out'' operations might form a session.

Several session guarantees can be made with high availability:

\vspace{.5em}\noindent\textbf{{Monotonic reads}} requires that, within
a session, subsequent reads to a given object ``never return any
previous values''; reads from each item progress according to a total
order. The ordering of reads should respect any externally observable
total ordering on transactions.

\vspace{.5em}\noindent\textbf{{Monotonic writes}} requires that each
session's writes become visible in the order they were submitted by
the client. Any order on transactions should also be consistent with
any precedence that a global observer would see.

\vspace{.5em}\noindent\textbf{{Writes Follow Reads}} requires that, if
a session observes an effect of transaction $T_1$ and subsequently
commits transaction $T_2$, then another session can only observe
effects of $T_2$ if it can also observe $T_1$'s effects (or later
values that supersede $T_1$'s).  Any order on transactions should
respect the reads-from order.\vspace{.5em}

The above guarantees can be achieved by forcing servers to wait to
reveal new writes until each write's respective dependencies are
fulfilled on all replicas. This mechanism effectively ensures that all
clients read from a globally agreed upon lower bound on the versions
written. This \textit{is} highly available as a client
will never block due to inability to find a server with a sufficiently
up-to-date version of a data item. However, it does not imply that
transactions will read their own writes or, in the presence of
partitions, make forward progress through the version history. The
problem is that, if a server becomes partitioned, under the highly
available model, we must handle the possibility that an unfortunate
client will be forced to issue her next requests against the
partitioned server.

The solution to this conundrum is to give up high availability in
favor of sticky availability. Sticky availability permits three
additional models, which we first define and then prove are
unachievable in a generic highly available system:

\vspace{.5em}\noindent\textbf{{Read your writes}} requires
that whenever a client reads a given data item after updating it, the
read returns the updated value (or a value with a higher ID).

\vspace{.5em}\noindent\textbf{{PRAM}} (Pipelined Random Access
Memory) provides the illusion of serializing each session's operations
and is the combination of monotonic reads, monotonic writes, and read
your writes~\cite{herlihy-art}.

\vspace{.5em}\noindent\textbf{{Causal
    consistency}}~\cite{causalmemory} is the combination of all of the
session guarantees~\cite{sessiontocausal} (alternatively, PRAM with
writes-follow-reads) and is also referred to by Adya as PL-2L
isolation~\cite{adya}).\vspace{.5em}


Read your writes is not achievable in a highly available
system. Consider a client that executes the following two transactions
in succession:
\begin{align*}
\small\vspace{-.5em}
T_1 &: w_x(1)
\\T_2 &: r_x(a)\vspace{-.5em}
\end{align*}
If the client executes $T_1$ against a server that is partitioned from
the rest of the other servers, the server must allow $T_1$ to
commit. If the client executes $T_2$ against the same (partitioned)
server, then it will be able to read its writes. However, if the
network topology shifts and the client can only contact a different
server that is partitioned from the server that executed $T_1$, then
the client will be unable to read its own writes and the system will
have to either stall indefinitely to allow the client to read her
writes (violating transactional availability) or will have to
sacrifice read your writes guarantees. However, if the client remains
sticky with the server that executed $T_1$, then we can disallow this
scenario. Accordingly, read your writes, and, by proxy, causal
consistency and PRAM require stickiness. Read your writes is provided
by default in a sticky system, while the remaining causality and PRAM
guarantees can be accomplished with the algorithms for achieving the
remaining session guarantees.

\subsubsection{Additional HAT Guarantees}

In this section, we briefly discuss additional noteworthy guarantees
achievable by HAT systems.

\vspace{0.5em}
\noindent{\textbf{Consistency}} A HAT system can make limited
application-level consistency guarantees. It can often execute
commutative and logically monotonic~\cite{calm} operations without the
risk of invalidating application-level integrity constraints. We do
not attempt to sketch the entire space of \textit{application-level}
consistency properties that are achievable in HAT systems (see
Section~\ref{sec:futurework}) but we specifically evaluate TPC-C
transaction semantics under HAT consistency guarantees in
Section~\ref{sec:evaluation}.

\vspace{.5em}\noindent{\textbf{Durability}} A client requiring that its
transactions' effects survive $F$ server faults requires at least
$F+1$ non-failing nodes.

\vspace{.5em}\noindent{\textbf{Convergence}} To require that the
system propagates writes between replicas, we can require convergence,
or eventual consistency for each data item: in the absence of new
mutations to a data item, in the absence of partitions, all servers
should eventually agree on the value for each item. This is typically
accomplished by any number of anti-entropy protocols, which
periodically update neighboring servers with the latest value for each
data item~\cite{antientropy}. Establishing a final value is related to
determining a total order on transaction updates, as in Read
Uncommitted.

\subsection{Unachievable HAT Semantics}
\label{sec:unachievable-hat}

While there are infinitely many HAT models
(Section~\ref{sec:futurework}), at this point, we have largely
exhausted the range of achievable, previously defined (and useful)
semantics that are available to HAT systems. Before summarizing our
possibility results, we will present impossibility results for HATs,
also defined in terms of previously identified isolation and
consistency anomalies. Most notably, it is impossible to
prevent Lost Update or Write Skew in a HAT system.

\subsubsection{Unachievable ACID Isolation}

In this section, we demonstrate that preventing Lost Update and Write
Skew---and therefore providing Snapshot Isolation, Repeatable Read,
and one-copy serializability---inherently requires unavailability.

Berenson et al. define \textit{Lost Update} as when one
transaction $T1$ reads a given data item, a second transaction $T2$
updates the same data item, then $T1$ modifies the data item based on
its original read of the data item, ``missing'' or ``losing'' $T2$'s
newer update. Consider a database containing only the following
transactions:
\begin{align*}
\small\vspace{-.5em}
T_1 &: r_x(a) w_x(a+2)
\\T_2 &: w_x(2)\vspace{-.5em}
\end{align*}
If $T_1$ reads $a=1$ but $T_2$'s write to $x$ precedes $T_1$'s write
operation, then the database will end up with $a=3$, a state that
could not have resulted in a serial execution due to $T_2$'s
``Lost Update.''

It is impossible to prevent Lost Update in a highly available
environment. Consider two clients who submit the following $T_1$ and
$T_2$ on opposite sides of a network partition:
\begin{align*}
\small\vspace{-.5em}
T_1 &: r_x(100)~w_x(100+20=120)
\\T_2 &: r_x(100)~w_x(100+30=130)\vspace{-.5em}
\end{align*}
Regardless of whether $x=120$ or $x=130$ is chosen by a replica, the
database state could not have arisen serial execution of $T_1$ and
$T_2$.\footnote{In this example, we assume that, as is standard in
  modern databases, replicas accept values as they are written (i.e.,
  register semantics). This particular example could be made
  serializable via the use of commutative updates
  (Section~\ref{sec:evaluation}) but the problem persists in the
  general case.}  To prevent this from happening, either $T_1$ or
$T_2$ should not have committed. Each client's respective server might
try to detect that another write occurred, but this requires knowing
the version of the latest write to $x$. In our example, this reduces
to a requirement for linearizability, which is, via Gilbert and
Lynch's proof of the CAP Theorem, provably unachievable with high
availability~\cite{gilbert-cap}.

\textbf{Write Skew} is a generalization of Lost Update to multiple
keys. It occurs when one transaction $T1$ reads a given data item $x$,
a second transaction $T2$ reads a different data item $y$, then $T1$
writes to $y$ and commits and $T2$ writes to $x$ and commits. As an
example of Write Skew, consider the following two transactions:
\begin{align*}
\small
T_1 &: r_y(0)~w_x(1)
\\T_2 &: r_x(0)~w_y(1)
\end{align*}
As Berenson et al. describe, if there was an integrity constraint
between $x$ and $y$ such that only one of $x$ or $y$ should have value
$1$ at any given time, then this write skew would violate the constraint (which is preserved in serializable executions). Write skew is a somewhat
esoteric anomaly---for example, it does not appear in
TPC-C~\cite{snapshot-serializable}---but, as a generalization of Lost
Update, it is also unavailable to HAT systems.

Their need to prevent Lost Update means that Consistent Read, Snapshot
Isolation, and Cursor Stability guarantees are all unavailable.
Repeatable Read (defined by Gray, Berenson et al., and Adya) and
One-Copy Serializability need to prevent both Lost Update and Write
Skew. These prevention requirements mean that these models are
inherently unavailable.

\subsubsection{Unavailable Recency Guarantees}

Distributed data storage systems often make various recency guarantees
on reads of data items. As we have discussed, one of the most famous
is linearizability~\cite{herlihy-art}, which states that reads will
return the last completed write to a data item, and there are several
other (weaker) variants such as safe and regular register
semantics. When applied to transactional semantics, the combination of
one-copy serializability and linearizability is called \textit{strong
  (or strict) one-copy serializability}~\cite{adya} (e.g.,
Spanner~\cite{spanner}). It is also common, particularly in systems
that allow reading from masters and slaves, to provide a guarantee
such as ``read a version that is no more than five seconds out of
date'' or similar. Unfortunately, an indefinitely long partition can
force an available system to violate any recency bound, so recency
bounds are not enforceable by HAT systems.

\subsection{Summary}
\label{sec:hat-summary}

As we summarize in Table~\ref{table:hatcompared}, a wide range of
isolation levels are achievable in HAT systems, including
transactional atomicity, cut isolation, and several session
guarantees. With sticky availability, a system can achieve read your
writes guarantees and PRAM and causal consistency. However, many other
prominent models, such as Snapshot Isolation, One-Copy
Serializability, and Strict Serializability cannot be achieved due to
the inability to prevent Lost Update and Write Skew phenomena.

We illustrate the hierarchy of available, sticky available, and
unavailable consistency models we have discussed in
Figure~\ref{fig:hatcompared}. Many models are simultaneously
achievable, but we find several particularly compelling. If we combine
all sticky-HAT guarantees, we have transactional, causal snapshot
reads (i.e., Causal Transactional Predicate Cut Isolation). If we
combine TA and P-CI, we have transactional snapshot reads. We can
achieve RC, MR, and RYW by simply sticking clients to servers. We can
also combine unavailable models---for example, an unavailable system
might provide PRAM and One-Copy
Serializability~\cite{daudjee-session}.

To the best of our knowledge, this is the first unification of
database ACID transactions, distributed consistency, and session
guarantee models. Interestingly, strong one-copy serializability
subsumes all other models, while considering the (large) power set of
all compatible models (e.g., the diagram depicts 96 possible HAT
combinations) hints at the vast expanse of consistency models found in
the literature. This taxonomy is not exhaustive, but we believe it
lends substantial clarity into the relationships between a large
subset of the prominent ACID and distributed consistency
models. Additional read/write transaction semantics that we have
omitted should be easily classifiable based on the available
primitives and HAT-incompatible anomaly prevention we have already
discussed.

 \newcommand{\lostupdate}{$^\dagger$}
 \newcommand{\rwskew}{$^\ddagger$}
 \newcommand{\linearizable}{$^\oplus$}

\begin{table}[t!]
\begin{tabular}{| c | p{6cm} | }\hline
HA & Transactional Atomicity (TA), Read Uncommitted (RU), Read
Committed (RC), Item Cut Isolation (I-CI), Predicate Cut Isolation
(P-CI), Monotonic Reads (MR), Monotonic Writes (MW), Writes Follow
Reads (WFR)\\\hline Sticky-HA & Read Your Writes (RYW), PRAM,
Causal\\\hline Unavailable & Cursor Stability (CS)\lostupdate,
Snapshot Isolation (SI)\lostupdate, Repeatable Read
(RR)\lostupdate\rwskew, One-Copy Serializability
(1SR)\lostupdate\rwskew, Recency\linearizable, Safe\linearizable,
Regular\linearizable, Linearizability\linearizable, Strong
1SR\lostupdate\rwskew\linearizable \\\hline
\end{tabular}
\caption{Summary of highly available, sticky highly available, and
  unavailable models considered in this paper. Unavailable models are
  labeled by cause of unavailability: preventing lost
  update\lostupdate, preventing write skew\rwskew, and requiring
  recency guarantees\linearizable.}
\label{table:hatcompared}
\end{table}

\begin{figure}[t!]
\centering
\begin{tikzpicture}[scale=0.8]
  \tikzstyle{sticky}=[rectangle,draw=blue!50,fill=blue!20,thick]
  \tikzstyle{noha}=[ellipse,draw=red!50,fill=red!20,thick, inner sep=0pt,minimum size=12pt]

  \tikzstyle{every node}=[font=\small]

 \node[draw=none,fill=none] (ici) at (1.2, 0) {I-CI};
 \node[draw=none,fill=none] (pci) at (1.65, 1.2) {P-CI};
 \node[draw=none,fill=none] (rc) at (-1.2, .8) {RC};
 \node[draw=none,fill=none] (ru) at (-1.2, 0) {RU};

 \node[draw=none,fill=none] (ta) at (-.2, 1.3) {TA};

 \node[draw=none,fill=none] (mr) at (3.6, 0) {MR};
 \node[draw=none,fill=none] (mw) at (4.8, 0) {MW};
 \node[draw=none,fill=none] (wfr) at (2.4,0) {WFR};
 \node at (6.1,0) [sticky] (ryw) {RYW};

 \node[noha](recency) at (7.7, 0) {recency};
 \node[noha](safe) at (7.7, 1) {safe};
 \node[noha](regular) at (7.7, 2) {regular};
 \node[noha](linearizable) at (7.7, 3) {linearizable};
 \node at (4.8, 2) [sticky] (causal) {causal};
 \node at (4.8, 1) [sticky] (pram) {PRAM};
 \node[noha] (cs) at (-1.2, 1.8) {CS};
 \node[noha] (rr) at (-.2, 2.7) {RR};
 \node[noha] (si) at (2.2, 2.4) {SI};
 \node[noha] (1sr) at (1.2, 3.2) {1SR};
 \node[noha] (ssr) at (3.85, 3.6) {Strong-1SR};

 \draw [->, red] (recency) -- (safe);
 \draw [->, red] (safe) -- (regular);
 \draw [->, red] (regular) -- (linearizable);
 \draw [->, red] (linearizable) -- (ssr);
 \draw [->, red] (1sr) -- (ssr);
 
 \draw [->] (ru) -- (rc);
 \draw [->] (rc) -- (ta);
 \draw [->] (ici) -- (pci);

 \draw [->, blue] (mr) -- (pram);
 \draw [->, blue] (mw) -- (pram);
 \draw [->, blue] (wfr) -- (causal);
 \draw [->, blue] (ryw) -- (pram);
 \draw [->, blue] (pram) -- (causal);

 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (ru) -- (mr);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (rc) -- (ta);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (ici) -- (ta);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (pci) -- (ta);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (rc) -- (mr);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (pci) -- (mr);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (ici) -- (mr);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (ta) -- (ru);
 %\draw[snake=coil, blue, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (ru) -- (causal);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (mr) -- (mw);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (wfr) -- (mw);
 %\draw[snake=coil, blue, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (wfr) -- (ryw);

 \draw [->, red] (rc) -- (cs);
 \draw [->, red] (cs) -- (rr);
 \draw [->, red] (pci) -- (si);
 \draw [->, red] (ici) -- (rr);
 \draw [->, red] (rr) -- (1sr);
 \draw [->, red] (si) -- (1sr);
 \draw [->, red] (ta) -- (si);
 \draw [->, red] (ta) -- (rr);
 \draw [->, red] (causal) -- (linearizable);
 \draw [->, red] (ryw) -- (safe);

\end{tikzpicture}
\label{fig:hat-order}
\caption{Partial ordering of HAT, sticky HAT (in boxes, blue), and
  unavailable models (circled, red) from
  Table~\protect\ref{table:hatcompared}. Directed edges represent
  ordering by model strength. Incomparable models can be
  simultaneously achieved, and the availability of a combination of
  models has the availability of the least available individual
  model.}
\label{fig:hatcompared}
\end{figure}


\subsection{Additional Discussion}
\label{sec:discussion}

In this section, we discuss several subtleties in our results,
specifically addressing model composition, transactional atomicity
versus linearizability, and benefits of stickiness.

\vspace{.5em}\noindent\textbf{Model Composition} Choosing between
combinations of compatible guarantees requires care. Consider the
following transactions:
\begin{align*}
\small\vspace{-.5em}
T_1 &: w_x(1)~w_y(1)
\\T_2 &: w_x(2)~w_y(2)
\\T_3 &: r_x(a)~r_y(b)\vspace{-.5em}
\end{align*}
If we want to guarantee both cut isolation and transactional atomicity
and the system only executes $T_1$, $T_2$, and $T_3$, then $T_3$ needs
to read $a=b=\bot$, $a=b=1$, or $a=b=2$. This means that either the
implementation should frequently return $\bot$ (definitely undesirable
and possibly non-convergent), keep multiple versions of each data item
(necessitating potentially complicated distributed garbage
collection), or use pre-declared read sets to fetch a consistent cut
of keys before each transaction begins to execute. Using client-side
caching can alleviate some of these challenges~\cite{bolton, swift},
but then the system becomes sticky high available.

Composition cost may also vary by combination. For instance, Charron-Bost
 proved that, to capture causality between $N$ communicating
processes, standard vector-based approaches face an upper bound of
$O(N)$ storage per write~\cite{charron-bost}. This means that, with
$100K$ clients, each write might be accompanied by $100K$ timestamps
per vector. This is difficult to scale. By compromising on
availability (e.g., treating a datacenter as a linearizable cluster),
this overhead can be reduced~\cite{eiger}, but it is much
cheaper to provide, say, read your writes, than full causal
consistency.

\vspace{.5em}\noindent\textbf{Linearizability and Transactional
  Atomicity} The relationship between linearizability and
transactional atomicity is non-obvious. TA dictates that writes to
multiple keys across multiple servers are made visible to readers all
at once, while linearizability dictates that writes to a single key on
multiple servers are made visible to all readers at once---what is
different? First, in linearizable (and safe and regular) systems,
writes are made visible to clients \textit{immediately} after they
finish. With transactional atomicity, there is no recency
guarantee. Second, in linearizable systems, all clients see all writes
at the same time. With TA as defined here, clients may see writes at
different times depending on which replicas they contact. We are not
aware of an analogous model in the distributed systems
literature. Accordingly, despite apparent similarities, TA is
incomparable with and much cheaper (by availability standards) than
linearizability.

\vspace{.5em}\noindent\textbf{Visibility and Stickiness} Sticky
availability can improve write \textit{visibility}: clients will be
able to safely read writes more quickly in a sticky available
system. In the model we discussed, it is possible to achieve several
properties like monotonic reads in a highly available system by
waiting to reveal a write until all servers have seen it and its
relevant dependencies. However, this incurs severe visibility
penalties---new writes will not become visible to clients in the
presence of partitions. A client that does not want to guarantee
read-your-writes (due to the sticky availability requirement) may
still wish to read other clients' writes with timeliness.
