
\section{Highly Available Transactions}
\label{sec:hats}

\begin{quote}
``...one should not throw out the C so quickly, since there are real
  error scenarios where CAP does not apply and it seems like a bad
  tradeoff in many of the other situations.''---Michael
  Stonebraker~\cite{stonebraker2010errors}
\end{quote}

HAT systems---providing transactions with transactional availability
with high availablity ($1$-available) or sticky high availability
($1$-sticky-available)---offer substantial latency and availability
benefits, yet they come with a cost to achievable semantics. In this
section, we delineate which of several ACID, distributed consistency,
and session consistency levels can be achieved with high availability
(e.g., transactional atomicity, variants of Repeatable Read isolation,
and many session guarantees) and sticky high availability (e.g.,
read-your-writes, PRAM, and causal consistency) and which cannot
(e.g., preventing Lost Update and Write Skew).

When possible, we draw on existing properties and definitions from the
database and distributed systems literature, providing a brief,
informal explanation of each guarantee and illustrate it with an
example. In particular, for our ACID guarantees, we draw largely on
Adya's dissertation~\cite{adya}, Berenson et al.'s 1995 critique of
the ANSI SQL Specification~\cite{ansicritique}, and the ANSI SQL
specification~\cite{ansi-sql}. We provide a set of formal definitions
and semantics in the extended version of this work.


\subsection{Achievable HAT Semantics}

To begin, we present achievable semantics, offering proof-of-concept
algorithms to demonstrate feasibility.

\subsubsection{ACID Isolation Guarantees}

To begin, Adya's \textit{Read Uncommitted} isolation requires that
transactions are totally ordered and that writes within transactions
are ordered consistently with this order (prohibiting ``Dirty
Writes,'' or $G0$)~\cite{adya}. If two transactions write to the same
set of data items, then the final database state cannot contain the
``earlier'' transaction's writes to the data items. For example, in
the below example, $T_3$ should eventually only read $a=b=1$ or
$a=b=2$ but not $a=2, b=1$ or $a=1, b=2$:
\begin{align*}
\small
T_1 &: w_x(1)~w_y(1)
\\T_2 &: w_x(2)~w_y(2)
\\T_3 &: r_x(a)~r_y(b)\\[-2em]
\end{align*}
The difference between this total order on transactions and the total
order required by serializability is that the Read Uncommitted total
order is completely arbitrary (i.e., an order must exist), whereas a
serializable total order must be equivalent to a serial
execution. Read Uncommitted is similar to the distributed systems
concept of replica convergence, in which all replicas for a data item
must end up with the same data item. This convergence is usually
achieved by picking a ``winning'' update, requiring a means of totally
ordering updates to each item; one can view Read Uncommitted as a
cross-item convergence property. It is easily achieved via applying
the same logical timestamp to each update in a transaction.

\textit{Read Committed} isolation requires that transactions do not
read uncommitted versions of data items (prohibiting both ``Dirty
Writes''---as above---and ``Dirty Reads'' phenomena; captured by
Adya's $G1a-c$, ANSI's $P1$, and Berenson's ``broad'' $P1$
(2.2)). For instance, in the example below, $T_3$ should never see
$a=1$, and, if $T_2$ aborts, $T_3$ should never read $a=3$:
\vspace{-.5em}
\begin{align*}
\small
T_1 &: w_x(1)~w_x(2)
\\T_2 &: w_x(3)\\
T_3 &: r_x(a)\\[-2em]
\end{align*}
It is fairly easy to prvent ``Dirty Reads'': if each transaction never
writes uncommitted data to the database, then transactions will never
read each others' dirty data. Clients can buffer their writes until
they commit, or, alternatively, can send them to servers, who will not
serve new writes until clients notify them that the writes have been
committed.

\textit{Repeatable Read} isolation is a contentious property. As
Berenson et al. discuss~\cite{ansicritique}, Gray's original
Repeatable Read lock-based implementation provides substantially
richer guarantees than those that are promised by the ANSI SQL
specification~\cite{gray-isolation}. Gray~\cite{gray-isolation},
Berenson et al.~\cite{ansicritique}, and Adya~\cite{adya} all
interpret Repeatable Read as providing serializability for all
operations except predicate-based reads. However, in its description
of Repeatabable Read, the ANSI SQL specification provides a useful
property which, although is not true to Gray's spirit of Repeatable
Read, is also commonly found in distributed consistency models: the
ANSI Repeatable Read guarantee requires that, along with observing
Read Committed isolation, each transaction will only read one version
of each data item that it did not itself produce (preventing ``Fuzzy
Read,'' or P2). In the example below, $T_3$'s must see $a=1$:
\begin{align*}
\small
T_1 &: w_x(1)
\\T_2 &: w_x(2)
\\T_3 &: r_x(1)~r_x(a)
\end{align*}
This isolation property is a literal interpretation of the phrase
``Repeatable Read'': unless a transaction modifies a given data item,
the value of the data item should not change during the
transaction. By itself, this property is rather weak---we can always
read \texttt{null} for each data item. However, when coupled with
additional properties, like transactional atomicity, it is stronger:
the repeated reads must obey additional ordering constraints. In fact,
distributed systems describe a ``consistent snapshot'' across a set of
related events as a \textit{cut} across a set of participants or, in
our case, data items. Accordingly, to capture the notion that the
transaction should read from a (non-changing) consistent cut over data
items (and to disambiguate from the aforementioned stronger Repeatable
Read properties), we call ANSI SQL Repeatable Read (preventing P2)
\textbf{Cut Isolation}.It is possible to satisfy cut isolation with
high availability by caching appropriate versions for a transaction's
reads or, alternatively, using multi-versioning on the server and
ensuring that each transaction's successive reads return the same
version of each data item.

We can further consider two variants of Cut Isolation. The first is
relatively straightforward and provides a cut across individual data
items. We will call this \textit{Item Cut Isolation}. The second
provides a cut over individual data items plus logical ranges of data
items, or predicate-based reads. This prevents the \textit{Phantom
  Problem}, whereby two successive predicate-based reads return
different data~\cite{gray-isolation}. We call predicate-based cut isolation, which prevents
Phantoms, \textit{Phantom Cut Isolation}.\footnote{Oracle provides an
  isolation level called ``Statement Level Read Consistency''; this is
  analogous to Phantom Cut Isolation at the level of a single
  statement. We do not consider this isolation model here as it is
  subsumed by standard Snapshot Isolation and we believe our
  discussion is easily extended to incorporate it.}

\subsubsection{ACID Atomicity Guarantees}

Transactional atomicity is core to ACID guarantees. Although, at least
by the acronym, it is not an ``isolation'' property, transactional
atomicity restricts transactions' ability to view the effects of
partially completed transactions. Under transactionational atomicity,
within each transaction, either all effects of another transaction are
observed, or none are (equivalently, once some of the effects of a
transaction are observed, all effects are observed). This is a
strictly stronger guarantee than Read Committed isolation: in Read
Committed, we can read a subset of committed writes whereas, with
transactional atomicity, we must read all of them. For example, if
$T_1$ commits, then, given transactional atomicity, $T_2$ must observe
$b=c=1$ (or later versions):
\vspace{-.5em}
\begin{align*}
\small
T_1 &: w_x(1)~w_y(1)~w_z(1)
\\T_2 &: r_x(a)~r_y(1)~r_x(b)~r_z(c)~\\[-1.5em]
\end{align*}
$T_2$ can also observe $a=\bot$ or $a=1$.

Perhaps perplexingly, discussions of transactional atomicity are
absent from existing discussions of weak isolation levels. This is
perhaps again due to the single-node context in which prior work was
developed: on a single server, atomicity is near-trivial to achieve
via lightweight locking and/or local concurrency control over data
items. (In this spirit, multi-item atomicity is common in so-called
``entity groups,'' which logically represent groups of co-located
items.) In contrast, in a distributed environment, atomicity over
arbitrary groups of items is considerably more difficult to achieve
with high availability. The key challenge is that different servers
responsible for subsets of a transaction's updates must coordinate
regarding when to reveal their writes to a transaction---without
violating availability due to, say, locking, or a missing
transactional write. Together with Cut Isolation, Transactional
Atomicity prevents Read Skew anomalies (Berenson et al.'s
A5A~\cite{ansicritique}).

Transactional atomicity is achievable in a HAT environment. To ensure
transactional atomicity, servers only need to know when all of a
transaction's updates are present on their respective servers but do
not need to know when all respective servers know when all of a
transaction's updates are present on their respective servers. This
subtle distinction corresponds to the difference between the
distributed systems concepts of consensus and reliable broadcast. The
former requires explicit agreement across servers, while the latter
simply requires that, if one server reveals a write, then all servers
reveal the write. We omit a full discussion of the algorithm and
instead refer the reader to the extended version of this paper.

\subsubsection{Session Guarantees}

So far, our models have not considered guarantees across transactions,
but, in the distributed systems context, many useful \textit{safety}
guarantees span multiple operations. There is no guarantee of forward
progression between transactions or any guarantees on the ordering
between transactions (other than that there exists some arbitrary
ordering). \textit{Session guarantees} are used to describe continuity
between groups of transactions issued by the same logical
user. Informally, a \textit{session} is a form of context that
consists of logical state that should persist between
operations. Users accessing a web site may want to experience a notion
of ``forward progress'' for the duration that they are logged in, so
all transactions submitted between ``log in'' and ``log out'' might
form a session. We can define sessions at several granularities (e.g.,
per-transaction, per-client, per-hour) but, for simplicity, will refer
to client-level guarantees in the discussion.

There are several guarantees that we can make with high availability:

\vspace{.25em}\noindent\textit{\textbf{Monotonic reads}} stipulates
that a client's subsequent reads to a given object ``never return any
previous values''; reads from each item progress forward in a total
order. The ordering of reads should respect any total ordering on
transactions.

\vspace{.25em}\noindent\textit{\textbf{Monotonic writes}} requires
that transactions from each individual client be serialized. Any order
on transactions should also respect the order in which each session
submitted transactions.

\vspace{.25em}\noindent\textit{\textbf{Writes Follow Reads}} requires
that, if a session reads a write originating from transaction $T_1$
then performs transaction $T_2$, then a session can only read $T_2$'s
writes if it can also read $T_1$'s prior write (or overwritten values
for $T_1$'s write). This requirement forms the basis for potential
causality: writes follow reads obeys the ``reads-from'' relation that
captures all events that influenced each
transaction~\cite{causalmemory}. Any transaction order should respect
the reads-from order (note that the total order on transactions
precludes reads-from cycles).

We can achieve the above guarantees by forcing servers to wait to
reveal new writes until their respective dependencies are fulfilled on
all replicas. In the case of Monotonic Reads, servers should only
reveal new writes to a data item when all other servers responsible
for the item have also seen them. Otherwise, a client might read a
newer version, subsequently become partitioned from its prior server,
and contact a comparatively out-of-date server. In the case of
Monotonic Writes, each new write should become visible only when each
client's prior write is visible everywhere, along with Monotonic Reads
guarantees. For Writes Follow Reads, a servers should similarly wait
to reveal items until their dependencies are present everywhere.

Effectively, this mechanism requires that clients only read from a
globally agreed upon lower bound on the versions written to the
system. This is highly available as a client will never block
due to inability to find a server with a sufficiently up-to-date
version of a data item. However, it does not imply that transactions
will read their own writes or provide much confidence in making
forward progress through the version history in the presence of
partitions. The problem is that, if a server is partitioned, under the
highly available model, we cannot discount the possibility that an
unfortunate client will be forced to issue her next requests against
the partitioned server!

The solution to this conundrum is to give up high availability in
favor of sticky availability. Sticky availability permits two
additional models, which we first define and then prove are
unachievable in a generic highly available system:

\noindent\textit{\textbf{Read your writes}} requires
that whenever a client reads a given data item after updating it, the
read returns the updated value (or a value with a higher ID).

\noindent\textit{\textbf{PRAM}} provides the illusion of serializing
each session's operations and is the combination of monotonic reads,
monotonic writes, and read your writes.

\noindent\textit{\textbf{Causal consistency}} is the combination of
all of the session guarantees~\cite{sessiontocausal} (alternatively,
PRAM and writes-follow-reads) and is also referred to as PL-2L
isolation~\cite{adya}).

Read your writes is unavailable in a highly available system. Consider
a client that executes the following two transactions in succession:
\vspace{-.5em}
\begin{align*}
\small
T_1 &: w_x(1)
\\T_2 &: r_x(a)
\end{align*}
If the client executes $T_1$ against a server that is partitioned from
the rest of the other servers, the server must allow $T_1$ to
commit. If the client executes $T_2$ against the same (partitioned)
server, then it will be able to read its writes. However, if the
network topology shifts and the client can only contact a different
server which is partitioned from the rest of the cluster, then the
client will not be able to read its own writes and the system will
have to either stall indefinitely to allow the client to read her
writes (violating transactional availability) or will have to
sacrifice read your writes. Accordingly, read your writes requires
stickiness, and, because causal consistency requires read your writes,
causal consistency also requires stickiness.

However, read-your-writes can be accomplished via client-side caching
and alternatively comes ``for free'' when requests are routed to a
sticky set of servers, while causality and PRAM can be accomplished
via sticky versions of the prior algorithms.

\subsubsection{Additional Guarantees}

A HAT system can make \textbf{limited consistency} guarantees. It can
execute commutative and logically monotonic~\cite{needed} operations
without the risk of invalidating (also monotonic) application-level
integrity constraints. Our goal in this paper is not to sketch the
entire space of consistency models that are achievable (see
Section~\ref{sec:futurework}, however we specifically evaluate TPC-C
transaction semantics under HAT consistency guarantees in
Section~\ref{sec:evaluation}.

As we briefly discussed in Section~\ref{sec:availability}, a client
requiring \textbf{durability} of surviving $F$ server faults requires
at least ($F+1$)-availability.

Finally, to require that servers propagate writes between replicas, we
can require \textbf{convergence}, or eventual consistency for each
data item: in the absence of new mutations to a data item, in the
absence of partitions, all servers eventually agree on the value for
each item. This is typically accomplished by any number of
anti-entropy protocols, which periodically update neighboring servers
with the latest value for each data item. This convergence property
should be considered mandatory given the spirit of existing literature
on isolation protocols: in all models we have encountered, the
database should behave as if there is some total order over
transactions. Indeed, non-convergent systems are remarkably difficult
to reason about.

\subsection{Unachievable HAT Semantics}
\label{sec:unachievable-hat}

While there are infinitely many HAT models
(Section~\ref{sec:futurework}), at this point, we have largely
exhausted the range of achievable, previously defined (useful) HAT
semantics. Before summarizing our possibility results, we will present
impossibility results for HATs, also defined in terms of previously
identified isolation and consistency anomalies.

\subsubsection{Unachievable ACID Isolation}

In this section, we demonstrate that preventing Lost Update and Write
Skew---and therefore providing Snapshot Isolation, Repeatable Read,
and Serializability---requires unavailability.

In the words of Berenson et al., \textbf{Lost Update} occurs when one
transaction $T1$ reads a given data item, a second transaction $T2$
updates the same data item, then $T1$ modifies the data item based on
its original read of the data item, ``missing'' or ``losing'' $T2$'s
newer update. Consider a database containing only the following
transactions:
\begin{align*}
\small
T_1 &: r_x(a) w_x(a+2)
\\T_2 &: w_x(2)
\end{align*}
If $T_1$ reads $a=1$ but $T_2$'s write to $x$ precedes $T_1$'s write
operation, then the database will end up with $a=3$---a state that
could not have resulted in a serial execution, arising from $T_2$'s
``Lost Update.''

It is impossible to prevent Lost Update in a highly available
environment. Consider two clients who submit the following $T_1$ and
$T_2$ on opposite sides of a network partition:
\begin{align*}
\small
T_1 &: r_x(100)~w_x(100+20=120)
\\T_2 &: r_x(100)~w_x(100+30=130)
\end{align*}
The final state of the database will converge to an execution that
could not have resulted from a serial excecution. To prevent this from
happening, either $T_1$ or $T_2$ should not have committed. Each
client's respective server might try to detect that another write
occurred, but this requires knowing the version of the latest write to
$x$. In this example, this reduces to a requirement for
linearizability, which is, via Gilbert and Lynch's proof of the CAP
Theorem, provably unachievable with high availabilty.

\textbf{Write Skew} is a generalization of Lost Update to multiple
keys. It occurs when one transaction $T1$ reads a given data item $x$,
a second transaction $T2$ reads a different data item $y$, then $T1$
writes to $y$ and commits and $T2$ writes to $x$ and commits. As an
example of Write Skew, consider the following two transactions:
\begin{align*}
\small
T_1 &: r_y(0)~w_x(1)
\\T_2 &: r_x(0)~w_y(1)
\end{align*}
As Berenson et al. describe, if there was an integrity constraint
between $x$ and $y$ such that only one of $x$ or $y$ should have value
$1$ at any given time, then this write skew would lead to a
non-serializable execution. Write skew in particular is a somewhat
esoteric anomaly---for example, it does not appear in
TPC-C~\cite{snapshot-serializable}---but, as a generalization of Lost
Update, is also unavailable.

The inability to prevent Lost Update means that Consistent Read,
Snapshot Isolation, and Cursor Stability guarantees are all
unavailable. The inability to prevent Lost Update or Write Skew means
that Repeatable Read, Conflict Serializability, and One-Copy
Serializability guarantees are unavailable.

\subsubsection{Unavailable Recency Guarantees}

Data storage engines make various recency guarantees on reads of data
items. As we have discussed, one of the most famous is
linearizability, which states that reads will return the last
completed write to a data item, and there are several other (weaker)
variants such as safe and regular register semantics. When applied to
transactional semantics, the combination of one-copy serializability
and linearizability is called \textit{strong (or strict) one-copy
  serializability} or \textit{externally consistent transactions}
(e.g., Google's Spanner~\cite{spanner}). It is also common,
particularly in systems that allow reading from masters and slaves, to
provide a guarantee such as ``read a version that is no more than five
seconds out of date'' or similar. Unfortunately, an indefinitely long
partition can cause violation of any recency bound.

\subsection{Summary}
\label{sec:hat-summary}

We summarize our results in Table~\ref{table:hatcompared}. A wide
range of isolation levels are achievable in a highly available system,
including transactional atomicity, cut isolation, and several session
guarantees. With sticky availability, a system can achieve PRAM
consistency, read your writes guarantees, and causal
consistency. However, many other prominent models, such as Snapshot
Isolation, One-Copy Serializability, and Strict Serializability cannot
be achieved due to the inability to prevent Lost Update and Write Skew phenomena.

We illustrate the hierarchy of available, sticky available, and
unavailble consistency models we have discussed in
Figure~\ref{fig:hatcompared}. Many models are simultaneously
achievable: for example, a HAT system can provide monotonic reads and
transactional atomicity or transactional atomicity and read your
writes and, for maximum semantic richness, can provide all of
Transactional Atomicity, Predicate Cut Isolation, Monotonic Reads,
Monotonic Writes, and Writes Follow Reads. An unavailable system might
provide PRAM and Serializability, as advocated by Salem and
Daudjee~\cite{needed}.

To the best of our knowledge, this is the first unification of
database ACID, distributed consistency, and session guarantee
models. Perhaps satisfactorily, strong one-copy serializability
subsumes all other models, while considering the (large) power set of
all compatible models (e.g., only considering client-centric session
consistency guarantees, the diagram depicts 32 possible HAT
combinations) hints at the vast expanse of consistency models found in
the literature. This taxonimization is \textit{not} exhaustive, but we
believe it lends substantial clarity into the relationships between a
large subset of the prominent ACID and distributed consistency
moels. Additional models of read/write transactions that we have
omitted should be easily classifiable based on the available
primitives and unavailable anomalies we have already discussed.

 \newcommand{\lostupdate}{$^\dagger$}
 \newcommand{\rwskew}{$^\ddagger$}
 \newcommand{\linearizable}{$^\oplus$}

\begin{table}
\begin{tabular}{| c | p{6cm} | }\hline
HA & Transactional Atomicity (TA), Read Uncommitted (RU), Read
Committed (RC), Item Cut Isolation (P-CI), Predicate Cut Isolation
(P-CI), Monotonic Reads (MR), Monotonic Writes (MW), Writes Follow
Reads (WFR)\\\hline Sticky-HA & Read Your Writes (RYW), PRAM,
Causal\\\hline Unavailable & Cursor Stability (CS)\lostupdate,
Snapshot Isolation (SI)\lostupdate, Repeatable Read
(RR)\lostupdate\rwskew, Conflict Serializability
(CSR)\lostupdate\rwskew, One-Copy Serializability
(1SR)\lostupdate\rwskew, Recency\linearizable, Safe\linearizable,
Regular\linearizable, Strong 1SR\lostupdate\rwskew\linearizable
\\\hline
\end{tabular}
\caption{Summary of highly available, sticky highly available, and
  unavailable models considered in this paper. Unavailable models are
  labeled by cause of unavailability: preventing lost
  update\lostupdate, preventing write skew\rwskew, and requiring
  recency guarantees\linearizable.}
\label{table:hatcompared}
\end{table}

\begin{figure}
\centering
\begin{tikzpicture}[scale=0.8]
  \tikzstyle{sticky}=[rectangle,draw=blue!50,fill=blue!20,thick]
  \tikzstyle{noha}=[ellipse,draw=red!50,fill=red!20,thick, inner sep=0pt,minimum size=12pt]

  \tikzstyle{every node}=[font=\small]



 \node[draw=none,fill=none] (ici) at (0, 0) {I-CI};
 \node[draw=none,fill=none] (pci) at (1.1, 1.3) {P-CI};
 \node[draw=none,fill=none] (rc) at (-1.2, .8) {RC};
 \node[draw=none,fill=none] (ru) at (-1.2, 0) {RU};

 \node[draw=none,fill=none] (ta) at (1.2, 0) {TA};
 \node[draw=none,fill=none] (mr) at (2.4, 0) {MR};
 \node[draw=none,fill=none] (mw) at (3.6, 0) {MW};
 \node[draw=none,fill=none] (wfr) at (6.3,0) {WFR};
 \node at (5,0) [sticky] (ryw) {RYW};

 \node[noha](recency) at (7.7, 0) {recency};
 \node[noha](safe) at (7.7, 1) {safe};
 \node[noha](regular) at (7.7, 2) {regular};
 \node[noha](linearizable) at (7.7, 3) {linearizable};
 \node at (3.6, 2) [sticky] (causal) {causal};
 \node at (3.6, 1) [sticky] (pram) {PRAM};
 \node[noha] (cs) at (-1.2, 1.8) {CS};
 \node[noha] (rr) at (-.8, 2.7) {RR};
 \node[noha] (si) at (2.2, 2.4) {SI};
 \node[noha] (s) at (0, 3.5) {CSR};
 \node[noha] (sr) at (2.2, 3.6) {1SR};
 \node[noha] (ssr) at (4.95, 3.7) {Strong-1SR};

 \draw [->, red] (recency) -- (safe);
 \draw [->, red] (safe) -- (regular);
 \draw [->, red] (regular) -- (linearizable);
 \draw [->, red] (linearizable) -- (ssr);
 \draw [->, red] (sr) -- (ssr);
 
 \draw [->] (ru) -- (rc);
 \draw [->] (ici) -- (pci);

 \draw [->, blue] (mr) -- (pram);
 \draw [->, blue] (mw) -- (pram);
 \draw [->, blue] (wfr) -- (causal);
 \draw [->, blue] (ryw) -- (pram);
 \draw [->, blue] (pram) -- (causal);

 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (ru) -- (mr);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (rc) -- (ta);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (ici) -- (ta);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (pci) -- (ta);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (rc) -- (mr);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (pci) -- (mr);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (ici) -- (mr);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (ta) -- (ru);
 %\draw[snake=coil, blue, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (ru) -- (causal);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (mr) -- (mw);
 %\draw[snake=coil, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (wfr) -- (mw);
 %\draw[snake=coil, blue, segment aspect=0, segment amplitude=.75pt, segment length=2pt] (wfr) -- (ryw);

 \draw [->, red] (rc) -- (cs);
 \draw [->, red] (cs) -- (rr);
 \draw [->, red] (pci) -- (si);
 \draw [->, red] (ici) -- (rr);
 \draw [->, red] (rr) -- (s);
 \draw [->, red] (si) -- (s);
 \draw [->, red] (ta) -- (si);
 \draw [->, red] (ta) -- (rr);
 \draw [->, red] (s) -- (sr);
 \draw [->, red] (causal) -- (sr);

\end{tikzpicture}
\label{fig:hat-order}
\caption{HAT, sticky HAT (in boxes), and unavailable models (circled)
  from Table~\protect\ref{table:hatcompared} compared
  graphically. Directed edges represent ordering by model
  strength. Models that do not share a common ancestor can be
  simultaneously achieved, and the resulting availability is that of
  the weakest available model in the combination.}
\label{fig:hatcompared}
\end{figure}


\subsection{Discussion}
\label{sec:discussion}

In this section, we discuss several subleties we have mostly elided up
to this point, specifically addressing model composition,
transactional atomicity versus linearizability, and stickiness
requirements.

\noindent\textbf{Model Composition} may be non-trivial. Consider the following transactions:
\begin{align*}
\small
T_1 &: w_x(1)~w_y(1)
\\T_2 &: w_x(2)~w_y(2)
\\T_3 &: r_x(a)~r_y(b)
\end{align*}
If we want to guarantee both cut isolation and transactional atomicity
and these are the only transactions that are executed by the system,
then $T_3$ needs to read $a=b=\bot$, $a=b=1$, or $a=b=2$. This means
that either the implementation should frequently return null
(definitely undesirable and possibly non-convergent), keep multiple
versions of each data item (necessitating potentially complicated
distributed garbage collection), or use pre-declared read sets to
fetch a consistent cut of keys before the transaction begins to
execute. Using client-side caching can alleviate some of these
challenges, but then the system becomes sticky high available.

Composition cost also varies by property. For instance, Charron-Bost
has proven that, to capture causality between $N$ communicating
processes, standard vector-based approaches face an upper bound of
$O(N)$~\cite{charron-bost} storage per write. This means that, with
$100,000$ clients, each write might be accompanied by $100,000$
timestamps per vector. This is difficult to scale. By compromising on
availability (e.g., treating a datacenter as a linearizable cluster),
this overhead can be reduced~\cite{cops, eiger}, but it is much
cheaper to provide, say, read your writes, compared to causality.

\noindent The relationship between \textbf{Transactional Atomicity and
  Linearizability} is non-obvious. One might attempt to find a mapping
between the two: Transactional Atomicity dictates that writes to
multiple keys across multiple servers are made visible to readers all
at once, while Linearizability dictates that writes to a single key on
multiple servers are made visible to all readers at once. The
difference is two-old. First, in linearizable (and safe and regular)
systems, writes are made visible to all clients \textit{immediately}
after they finish. With Transactional Atomicity, there is no recency
requirement. Second, in linearizabile systems, all clients see all
writes at the same time. With Transactional Atomicity, clients see
writes at different times depending on which replicas they contact. We
are not aware of an analogous model in the distributed systems
literature. Accordingly, despite apparent similarities, Transactional
Atomicity is much weaker than Linearizability.

\noindent There is a substantial \textbf{visibility benefit to sticky
  availability}: writes can safely become visible to readers much more
quickly in a sticky available system. In the model we discussed, it is
possible to achieve several properties like monotonic reads in a
highly available system by waiting until all servers have seen a
write. However, this incurs severe visibility penalties---new writes
will not become visible to clients in the presence of partitions. A
client that does not want to guarantee read-your-writes (due to the
sticky availability requirement) may still wish to read other clients'
writes with timeliness. Accordingly, a system may forgo high
availability in order to provide better visibility. Alternatively, a
system may provide stickiness in the absence of partitions, then
``lose'' session guarantees by having clients reconnect to an
available replica. This latter strategy provides session guarantees in
the absence of partitions but, to provide availability, sacrifices
them in the event of partitions.

